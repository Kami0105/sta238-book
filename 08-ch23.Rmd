# Supplement to Chapter 23

This chapter implements much of the analysis shown in chapter 23 of 
A Modern Introduction to Probability and Statistics. R code is given for the
simple textbook datasets used in the book, and then the concepts are
illustrated on real data.

All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/.

```{r load-tidy-1,message=FALSE,warning=FALSE}
library(tidyverse)
```

## Confidence Intervals for the Mean (Chapter 23)

### Simulation 

Let's first implement the simulation from page 344. We'll generate a bunch of samples
from a $\text{N}(0,1)$ distribution, compute their confidence intervals, and plot them

```{r normsim-1}
set.seed(432432432)
# How many samples to generate
B <- 50
# Sample size of each
n <- 20
# Confidence level
conf <- .9
# Critical value- the book just gives this as 1.729
# This actually took me a minute to figure out... so make sure
# you get what I'm doing here:
critval <- qt(1 - (1-conf)/2,df = n-1)
# Perform the simulation
confints <- 1:B %>%
  # Generate the samples
  map(~rnorm(n,0,1)) %>%
  # Compute the confidence intervals
  map(~c("mean" = mean(.x),
         "lower" = mean(.x) - critval * sd(.x)/sqrt(n),
         "upper" = mean(.x) + critval * sd(.x)/sqrt(n))
      ) %>%
  # Put them in a dataframe
  reduce(bind_rows) %>%
  # Add a row index, for purposes of plotting
  mutate(id = 1:B)

# Compute the proportion that don't contain zero
scales::percent(mean(confints$upper < 0 | confints$lower > 0))

# Plot them. Run this code layer by layer
# to understand what each part does.
confints %>%
  ggplot(aes(x = id)) +
  theme_classic() +
  geom_point(aes(y = mean),pch = 21,colour = "black",fill = "orange",size = 1) +
  geom_errorbar(aes(ymin = lower,ymax = upper),size = .1) +
  geom_hline(yintercept = 0,colour = "red",linetype = "dotdash") +
  scale_y_continuous(breaks = seq(-1,1,by=.2)) +
  coord_flip() +
  theme(axis.title.y = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank()) +
  labs(y = "")


```

**Exercise**: re-run this experiment several times with different random seeds. What
kind of empirical coverage probabilities---the proportion of intervals that don't contain zero---do you
get? What about if you raise the sample size to `n = 100`? What about if you raise the
number of simulations to `B = 1000`?

### Gross calorific value measurements for Osterfeld 262DE27

The Osterfield data is made available with the book. Its file name is misspelled,
so be careful:

```{bash oster-1}
head data/MIPSdata/grosscalOsterfeld.txt
wc -l data/MIPSdata/grosscalOsterfeld.txt
```

Read it in. I'm leaving this as an exericse (note: not because I'm lazy, I still had to
write the code. It's for your "learning" or whatever). You should get the following:

```{r oster-2,echo = FALSE}
osterfield <- readr::read_csv(
  file = "data/MIPSdata/grosscalOsterfeld.txt",
  col_names = "calorific_value",
  col_types = "n"
)
```

```{r oster-3}
glimpse(osterfield)
```

Recreate the confidence interval in the book:

```{r oster-4}
# Compute the sample mean and size
xbar <- mean(osterfield$calorific_value)
n <- nrow(osterfield)
# The population standard deviation, and the critical value/confidence level
# are given as:
sigma <- .1
conf <- .95
# Make sure to UNDERSTAND this calculation:
critval <- qnorm(1 - (1-conf)/2) # 1.96
# Compute the interval
c(
  "lower" = xbar - critval * sigma/sqrt(n),
  "upper" = xbar + critval * sigma/sqrt(n)
)
```

### Gross calorific value measurements for Daw Mill 258GB41

As an exercise, now recreate the confidence interval in the book for the
Daw Mill sample. Read the data in from file `grosscalDawMill.txt`, call it
`dawmill`. You can compute the sample standard deviation and appropriate
critical value as follows:

```{r daw-1,echo = FALSE}
dawmill <- readr::read_csv(
  file = "data/MIPSdata/grosscalDawMill.txt",
  col_names = "calorific_value",
  col_types = "n"
)
```

```{r daw-2}
s <- sd(dawmill$calorific_value)
critval <- qt(1 - (1-conf)/2,df = nrow(dawmill) - 1)
```

You should get:

```{r daw-3,echo = FALSE}
xbar <- mean(dawmill$calorific_value)
n <- nrow(dawmill)
c(
  "lower" = xbar - critval * s/sqrt(n),
  "upper" = xbar + critval * s/sqrt(n)
)
```

### Bootstrap Confidence Intervals

First, let's simulate a dataset to illustrate this idea and so we can compare
the bootstrap and analytical answers.

```{r boot-1}
set.seed(43547803)
B <- 2000
n <- 5000
# Simulate one dataset
ds <- rnorm(n,0,1)
# Values
conf <- .95
critval <- qnorm(1 - (1 - conf)/2)
# Now resample from it and calculate studentized statistics
resampledstats <- 1:B %>%
  map(~sample(ds,n,replace = TRUE)) %>%
  map(~c(mean(.x) - mean(ds))/(sd(.x)/sqrt(n))) %>%
  reduce(c)

# The confidence limits are obtained from the sample quantiles:
conflim <- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2))
# Here's a plot that illustrates what these look like:
tibble(x = resampledstats) %>%
  ggplot(aes(x = x)) +
  theme_classic() +
  geom_histogram(aes(y = ..density..),colour = "black",fill = "lightgrey",bins = 100) +
  geom_vline(xintercept = conflim[1],colour = "orange",linetype = "dotdash") +
  geom_vline(xintercept = conflim[2],colour = "orange",linetype = "dotdash") +
  stat_function(fun = dnorm,args = list(mean = mean(ds),sd = sd(ds)),colour = "blue") +
  labs(title = "Resampled student statistics and empirical confidence limits",
       subtitle = "A normal distribution (blue curve) fits well",
       x = "",y = "")
```

I deliberately chose a large sample size and number of bootstrap samples to make the
results look good. I encourage you to change these numbers to try and break this
simulation.

The bootstrap-resampled confidence limits are close to the truth:

```{r boot-2}
conflim
qnorm(c((1-conf)/2,1 - (1-conf)/2))
```

Let's apply this to the software data. Oddly, I get different values for the mean,
standard deviation, and sample size than the book reports. If you figure out why,
I will give you a $\$10$ Tim card. The differences aren't meaningful enough to 
affect the presentation of these ideas.

```{r software-1-1}
# Read it in:
software <- readr::read_csv(
  file = "data/MIPSdata/software.txt",
  col_names = "time",
  col_types = "n"
)

B <- 1000 # Same as book
n <- nrow(software)
mn <- mean(software$time)
ss <- sd(software$time)
conf <- .9
set.seed(821940379)
resampledstats <- 1:B %>%
  map(~sample(software$time,n,replace = TRUE)) %>%
  map(~c(mean(.x) - mn)/(sd(.x)/sqrt(n))) %>%
  reduce(c)

# The confidence limits are obtained from the sample quantiles:
conflim <- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2))
# The confidence interval:
c(
  "lower" = mn + conflim[1] * ss/sqrt(n),
  "upper" = mn + conflim[2] * ss/sqrt(n)
)
```

**Exercise**: compute a $90\%$ confidence interval for the mean for the software
data assuming the data is normally distributed. This does NOT mean that you should
use a normal distribution for calculating the critical values-- if you don't understand
why, go back and read the "Variance Unknown" section on page 348. I got the following:

```{r boot-4}
critval <- qt(1 - (1-conf)/2,df = n -1)
c(
  "lower" = mn - critval * ss/sqrt(n),
  "lower" = mn + critval * ss/sqrt(n)
)
```

Does this lead to different conclusions in practice than the bootstrap interval?
