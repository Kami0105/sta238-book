# Statistics and Plots

## Introduction

Data only becomes information after it is analyzed. Analyzing data is extraordinarily difficult,
but is the only way to learn meaningfully about the world.

In this book, you will learn about analyzing data. You will learn how to make principled,
measured statements about the data on hand (*descriptive statistics*); how to 
use the data on hand to make statements about the underlying world, and how to
quantify the uncertainty in your statements (*inferential statistics*), and how
to use the data on hand to make judgements, with uncertainty quantification,
about what data you will see next (*predective statistics*).

This chapter focussed on **descriptive statistics**. This loosely refers to the set
of principles and tools that you use to tell a story using available data. The data
may be collected from a formal scientific experiment, or might be from a survey which
relies on a random sample of a population. It could exist already in a big database
at a bank or insurance company, or it could be scraped from the web. In all
cases, the sitation is: you have data, and you want to use it to *tell a story.*
  
## Descriptive Statistics
  
  Like with usual storytelling, how you *present* the data makes a big difference.
There is no one correct way to analyze any given dataset. You will make judgements,
and assumptions, and you will have to justify and explain them. These judgements
and assumptions can have a profound impact on the story you tell using your data.

For example, consider a famous dataset containing information on smoking and mortality.
The data is available in the `R` package `faraway`. We may load the package and data
and retrieve information on it as follows:
  
  ```{r load-smoke}
# install.packages("faraway") # Run this to install the faraway package
library(faraway) # Attach the faraway package
data("femsmoke") # Load the "femsmoke" data
# ?femsmoke # Run this to open the help page for the dataset.
```

Lines in `R` that begin with a pound sign (or "hashtag" for the younger, more hip reader), $\#$,
are comments and are not run. Remove the $\#$ to run the code.

We see from the help page, and associated reference to the paper in the *American Statistician*,
that the data comes from asking women in Newcastle, England, whether they smoke or not, 
and then following up in 20 years to see if they died. We can use these data to
observe any apparent association between smoking and mortality.

To investigate any such associations, we can simply look at the observed mortality rates
for smokers and non-smokers. This is our first **statistic**. What makes it a "statistic"?
  Formally, a **statistic** is any summary---or *function*---of observed data. Here we
are going to report the *mortality rate*---the proportion of people who died---for smokers,
and non-smokers. These are both statistics, and reporting these two statistics side
by side is our first example of an **exploratory data analysis**.

```{r mort-rate-1,warning=FALSE,message=FALSE}
# install.packages("tidyverse")
library(tidyverse) # We use a LOT of functions from this package.

# Compute the mortality rate for smokers and non-smokers.
# To do this, create a dataframe containing the numbers of smokers
# and non-smokers
smoker_numbers <- femsmoke %>% # The %>% operator lets you form sequences of operations
  group_by(smoker) %>% # group_by() makes all the following operations happen within groups
  summarize(num_people = sum(y)) # Count the number of people who are smokers and not smokers 
smoker_numbers

# Now, compute the number of people who died out of the smokers and non-smokers
# This looks the same as above, except we now filter() only the people who died.
smoker_numbers_dead <- femsmoke %>%
  filter(dead == "yes") %>% # Retains rows where dead == "yes" only
  group_by(smoker) %>%
  summarize(num_dead = sum(y))
smoker_numbers_dead

# Now, we join these two tables together and compute the mortality rates by group.
smoker_numbers %>%
  inner_join(smoker_numbers_dead,by = "smoker") %>% # Joins rows with the same value of "smoker"
  mutate(mort_rate = num_dead/num_people) # mutate() creates a new variable, which can be a function of the other variables in the dataframe.
```

See anything interesting?
  
  What went wrong? Why are we observing that smokers have a *lower* mortality rate than
non-smokers? Two possibilites immediately come to mind:
  
  1. The opposite is really true, however, we got a very unlikely dataset;
1. We made a mistake in our analysis.

We will address the first notion when we discuss **inferential statistics** later
in the course. The second notion is more immediate. Did we make a mistake?
  
  One thing we definitely did was ignore some present information. Specifically,
we also know how old the women were. How can we include this information in our
descriptive statistics? We can compute mortality rates by age:
  
  ```{r mort-rate-2,warning=FALSE,message=FALSE}
smoker_numbers_age <- femsmoke %>%
  group_by(smoker,age) %>% # Now we're grouping by smoker AND age. The rest of the code remains unchanged.
  summarize(num_people = sum(y))

smoker_numbers_age_dead <- femsmoke %>%
  filter(dead == "yes") %>%
  group_by(smoker,age) %>%
  summarize(num_dead = sum(y))

smoker_numbers_age %>%
  inner_join(smoker_numbers_age_dead,by = c("smoker","age")) %>% 
  mutate(mort_rate = num_dead/num_people)
```

Older people are more likely to die within the 20 year followup period. 
However, examining the raw counts of people in each group, we also see that 
*in these data, older people are less likely to smoke* than younger people. So
*in these data*, less smokers died, because less smokers were old, and more
old people died.

But was our first analysis wrong? No. Our first analysis was fine: we computed
the mortality rate in each group. The problem was in the reporting, or the way
we told the story. We didn't provide enough information when we said "the mortality
rate for smokers was lower than for non-smokers". We should have mentioned that
this is *averaging over all age groups*. Even when we include age in the analysis,
we ought to mention the fact that there are a whole lot of variables we *could*
have measured but didn't, and we are implicitly averaging over these too.

Before moving on, get some practice doing exploratory analysis with the following
exercises:
  
### Exercises
  
  1. How many non-smoking 18-24 year olds are there in the `femsmoke` data? Answer
first by reading the answer off of the table of mortality rates by age that was
just presented. Now compute the number yourself in the following two ways:
  
  a. Use `group_by()` to group the data by `smoker` and `age`; `summarize()` to
compute the number of people in each `smoker/age` group, and then `filter()` to
retain only `smoker == "yes"` and `age == "18-24"`.
a. Use `filter(smoker == "yes",age = "18-24")` to filter by both these variables,
then use `summarize()` to compute the total number of people.

1. What is the *relative risk* of mortality---the ratio of the mortality rates---for
smoking 18-24 year olds vs non-smoking 18-24 year olds? Compute the answer manually by
reading the numbers off the above table again. Then compute it using `R` by doing the
following:
  
  - Create two datasets using `filter()`: one containing smokers and one containing
non-smokers. `filter()` out only the 18-24 year olds. This gives you two datasets
each with only one row. For example, `smokers <- femsmoke %>% filter(smoker == "yes",age = "18-24")`.
- `inner_join()` the two datasets together, using `age` as the `by` variable:
  `smokers %>% inner_join(???,by = "age")`

1. *Advanced*: modify the above steps to create the following table of relative mortality rates. You should start from a cleaned up version of the mortality rate by age table:
  
  ```{r rel-mort-rate-2}
rates_by_age <- smoker_numbers_age %>%
  inner_join(smoker_numbers_age_dead,by = c("smoker","age")) %>% 
  mutate(mort_rate = num_dead/num_people) %>%
  ungroup() # The data was previously grouped, we don't want this anymore
```

Use `dplyr::select()` to remove and rename columns, see `?dplyr::select`.

```{r rel-mort-rate-1,echo = FALSE}
smokers <- rates_by_age %>%
  ungroup() %>%
  filter(smoker == "yes") %>%
  dplyr::select(age,smoker_mort_rate = mort_rate)

nonsmokers <- rates_by_age %>%
  filter(smoker == "no") %>%
  dplyr::select(age,nonsmoker_mort_rate = mort_rate)

smokers %>%
  inner_join(nonsmokers,by = "age") %>%
  mutate(relative_risk = smoker_mort_rate/nonsmoker_mort_rate)

```

## Extended example: the state of rental housing in Toronto

We saw a basic example in the `smoking` data of how the choices that we make
while analyzing a dataset can lead to different conclusions. The `smoking`
data is quite small, and there aren't that many choices to make. Let's take a
look at something more complicated, modern, and topical.

The RentSafeTO: Apartment Building Standards program is designed to help renters
in the city of Toronto make informed choices about where to live, and to enforce
a minimum standard of quality upon rental units within the city. With rents 
skyrocketing and home ownership not a reasonable option for most, having an
informed view of the rental market is imperative. It also helps keep leaders
accountable, specifically if we focus on social and community housing buildings.

Comprehensive and fairly clean data from the program, along with specific
information, is available at https://open.toronto.ca/dataset/apartment-building-evaluation/.
Data for the following were downloaded on `2019/09/16`. 

To start your analysis, go now and download the data
and open it in a spreadsheet and have a look. Familiarize yourselves with the
variable descriptions and how the data were collected. This somewhat tedious 
task is a first step of **any** data analysis, in academia, industry, government,
or wherever.

### Load the data

The data are stored in a `.csv` file, which stands for "comma-separated-values".
Storing data in a text file with a separator, usually a comma, is very common.
These are referred to as "flat files" in an industrial context, to distinguish
them from data stored in databases.

We may read the data into `R` using the `read_csv` function in the `readr` package.
The `readr` package is part of the `tidyverse` package that we used before, so if
you installed that package, you have it loaded.

```{r read-apartment-1}
# https://open.toronto.ca/dataset/apartment-building-evaluation/
# install.packages("readr")

# Read the data in. This means call the readr::read_csv() function, point it
# to where you saved the data on your computer, and then save the result to a
# variable. I am naming this variable 'apartmentdata'.
# Type ?readr::read_csv if you want to read about this function.
apartmentdata <- readr::read_csv(
  file = "./data/apartment-data/toronto-apartment-building-evaluations.csv"
)
```

The message displayed is telling you that `readr::read_csv()` guessed at what
kind of data were in each column, i.e. numbers, letters, dates, etc. You should
make sure, as I have while writing, that these are what you expect. You can
get a concise view of this dataset using the `glimpse` function in the `dplyr`
package, which is automatically loaded when you load the `tidyverse`:
  
  ```{r apartment-data-2}
glimpse(apartmentdata)
```

That's bigger than the `smoking` data! 3,446 rental apartment buildings, each with
32 factors measured. The buliding's address and Ward number are in there, which are
helpful for characterizing neighbourhoods.

### Do different wards have different quality housing?

A Ward is an administrative district within the city that has a single city 
counsellor. If I'm thinking about moving to, or within, Toronto, I want to know:
**Do different wards have different quality housing?**.

In order to address this question we need to decide on the following:

- **Variable of interest**. How do we *quantify* our research question? We need
to pick a *measure of quality*. Picking different measures can lead to different
conclusions.
- **Filters**. Do we look at all apartment buildings? Should we look only at those
built after, or before, a certain date? Only those that meet a certain minimum,
or maximum, standard of quality according to our definition? Are there any other
kinds of decisions we might have to consider?
- **Methods**. What kind of statistical tools should we use to address our research
question? We need to pick descriptive statistics to report, and decide whether 
we want to include other auxillary variables in the analysis.
- **Conclusions**. How do we report our results? Tables, charts, maps? Should we
include subjective, editorial commentary, or let the data speak for
themselves?

This is already overwhelming! Let's make an attempt at it. I propose:
  
- Our **variable of interest** should be `SCORE`, which you know (because you
read the documentation...) is the "overall score of the buliding". Higher is 
better. The actual formula is included in the documentation of the data.
- We will **filter** the data to only include buildings where `PROPERTY_TYPE == 'PRIVATE'`,
which will restrict our analysis to not include social housing. The quality of social
housing is an important social justice issue (that you will investigate in the exercises)
but it's somewhat separate (?) from the question of where to look for rental housing.
- Our **methods** will include looking at a table of average scores for each ward.
We will also look at whether older or newer buildings receive better scores. 
- We will summarize our **conclusions** through a subjective assessment of the 
above table of average scores.

With these decisions made, we may proceed with our analysis using the `tidyverse`
as follows:

```{r apartment-analysis-1}
# First, select only the columns you want
# This isn't strictly necessary but trust me, it makes 
# debugging WAY easier.
# I'm also renaming the columns so the dataframe looks prettier.
# Again, trust me. This stuff matters.
apartmentclean <- apartmentdata %>% 
  dplyr::select(ward = WARD,
                score = SCORE,
                property_type = PROPERTY_TYPE,
                year_built = YEAR_BUILT,
                address = SITE_ADDRESS
  )
glimpse(apartmentclean) # Much nicer!
# Apply filter(s).
apartmentfiltered <- apartmentclean %>%
  filter(property_type == "PRIVATE")
# When filtering, always compare the filtered and unfiltered data to ensure
# the result is as expected:
glimpse(apartmentclean)
glimpse(apartmentfiltered)
nrow(apartmentclean) - nrow(apartmentfiltered) # Dropped 567 rows.

# Now create the table of averages:
apartmentfiltered %>%
  group_by(ward) %>%
  summarize(avg_score = mean(score))

```

Bah! What happened? Why are there these `NA` values? 
  
  `NA` is the value `R` uses to mean "missing". We have to hope that whether a 
rental apartment building's `score` is missing *is not related* to what that score *is*,
that is, we hope apartments with higher or lower scores aren't missing more often.

We will ignore missingness for now. To do this, use the `na.rm = TRUE` option in
`mean`:

```{r apartment-analysis-2}
apartmentsummary <- apartmentfiltered %>%
  group_by(ward) %>%
  summarize(avg_score = mean(score,na.rm = TRUE))
apartmentsummary
```

This isn't a super friendly way of comparing these 26 numbers. We need some kind
od **visualization**, or **plot**, so we can take one look and get an idea of 
what is going on.

### Data visualization for description

In order to pick what kind of plot to make, we have to understand what types
of variables we are attempting to compare. Broadly speaking, variables can
either be **continuous** or **categorical**. 

A **continuous** variable consists of
numbers. You can always compare the *order* of two values of a continuous variable
(say whether one is bigger than the other), because its values are just numbers.

A **categorical** variable takes values from a finite (discrete) set. These values
can be anything. Categorical variables sometimes have values which can be compared,
like "big", "medium", "small"; but often the values are not inherently comparable.

In the case of the table of average scores by ward, we will consider `avg_score`
to be a **continuous** variable and `ward` to be a **categorical** variable. Wards
are labelled with numbers, yes, but they are not inherently comparable; they are
areas within the city. It wouldn't make sense to say that ward 2 were "less" than
ward 17. We could have labelled them "A", "B", "C",... and the meaning would
have been the same.

One common visualization used to compare values of a **continuous** variable
with values of a **categorical** variable is a **bar chart**. A bar chart contains
the values of the categorical variable on the x-axis and the values of the
continuous variable on the y-axis. You can take one look and see whether there is
any pattern in values of the continuous variable with respect to the categorical
variable.

We create a **bar chart** to visualize these summary statistics using the `ggplot2`
package. This is also included with the `tidyverse` (surprise!):

```{r barchart-apartment-1}
apartmentsummary %>%
  ggplot(aes(x = ward,y = avg_score)) + # Set up the variable mappings
  theme_light() + # Make it pretty
  geom_bar(stat = "identity",colour = "black",fill = "grey") + # Add the bars
  labs(title = "Average ABS score shows moderate variability across wards in Toronto",
       x = "Ward",
       y = "Average ABS Score") # Informative title
```

It looks like some wards are better than others. Or are they? Can we make any
definitive conclusions based on this?

### Further analysis: trends in quality over time

Let's go further and analyze some other interesting aspects of these data. I'm
interested in knowing: **Are newer buildings higher quality**? 

We have the `score`
and the `year_built`, and we'd like to investigate whether newer buildings (higher
`year_built`) have higher `scores`. We have another decision to make. We could
consider `year_built` to be a categorical variable, and make a bar chart. Or, 
we could consider it to be a continuous variable.

Because values of `year_built` are inherently comparable, and because *our research
question involves making such comparisons*, we will consider `year_built` to be
a continuous variable.

One type of plot used to compare continuous variables is a **scatterplot**. A 
scatterplot has continuous variables on the x- and y-axes, and draws a point (or
bubble) at each place in the two-dimensional plane where a datapoint occurs. We
can make this kind of plot in `ggplot2` as well. This time, we use the raw 
(well, cleaned and filtered) data:

```{r apartment-plot-2}
apartmentfiltered %>%
  filter(year_built > 1900) %>%
  ggplot(aes(x = year_built,y = score)) +
  theme_light() + 
  geom_point(pch = 21,colour = "black",fill = "grey") + # pch=21 makes the bubbles hollow, looks nice
  scale_x_continuous(breaks = seq(1900,2020,by=10)) + # Set the x-axis range
  labs(title = "Less rental buildings are being built recently, but they are of higher quality",
       x = "Year Built",
       y = "ABS Score")
```

Very interesting. You can clearly see the baby boom of the 1950's to 1970's, followed by a massive slowdown in construction during the economic slump in the 1980's, and a complete stop when rent control was introduced in 1991 (remember, these are *rental* buildings only). Then, we see a new wave of
rental building construction, and the new buildings seem to be of higher quality.

What are the highest and lowest quality rental buildings in Toronto?

```{r apartment-3}
# Get the 10 highest scoring buildings
apartmentfiltered %>%
  arrange(desc(score)) %>% # Sort the data, descending, by score
  slice(1:10) # Take the first ten- i.e. the top ten
```

Wow. I know where I want to live.

### Summary

We have seen how even something simple like trying to figure out whether
different areas of the city have different quality housing can require
a lot of decision making. And these decisions require expertise. By taking
a principled approach to learning **data analysis**, you are *empowering*
yourself to live a life that is better informed.

But notice that we didn't really answer any questions in this chapter. We
saw some rough patterns, but were they real? If we made different decisions, or if we *sampled different data*, would we have seen different patterns?

In order to understand what the problem is and how to approach it, we
need to take a more detailed look at the concept of **error**. This is the
subject of Chapter 2.

### Exercises

1. Read the documentation online and choose three variables that you find the most interesting. Reproduce the analysis of sections 1.3.2. and 1.3.3. using your favourite variables. Is there more or less variability across wards than with `score`?

1. What is the ward with the highest average score? In what ward is/are the building(s) with the highest score(s)? Is this the same ward, or not? Would you expect the ward with the highest average to also have the highest-scoring buildings? Repeat this question with the lowest scoring buildings instead of the highest.

1. If you live in a rental apartment, find it in these data. If not, find a friend's place. How does your building compare to other buildings in your ward? Does it score higher or lower? The `filter()` function is your friend here, or you can use `apartmentfiltered %>% arrange(SITE_ADDRESS) %>% print(n = Inf)` and then find yours in the list manually.

1. Combine the analyses of sections 1.3.2 and 1.3.3. with that of 1.3.4. Specifically, make a table and a boxplot of the average score by year. This means replace `ward` by `year_built` in the analysis of sections 1.3.2. and 1.3.3. Do your conclusions change when comparing with 1.3.4? Why or why not? Would you expect this to always be the case?

1. *Advanced*: analyze the quality of **social housing** in Toronto. Perform a similar analysis to what we performed here for `PROPERTY_TYPE == 'PRIVATE'`, but instead for `PROPERTY_TYPE %in% c('SOCIAL HOUSING','TCHC')` (equivalent to `PROPERTY_TYPE != 'PRIVATE'`). Does the quality of social housing in Toronto vary greatly across different wards? Is it improving or degrading over time? Do you think we have enough information here to definitively answer these questions?
