[
["section-supplement-to-chapter-23.html", "Chapter 9 Supplement to Chapter 23 9.1 Confidence Intervals for the Mean (Chapter 23)", " Chapter 9 Supplement to Chapter 23 This chapter implements much of the analysis shown in chapter 23 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. library(tidyverse) 9.1 Confidence Intervals for the Mean (Chapter 23) 9.1.1 Simulation Let’s first implement the simulation from page 344. We’ll generate a bunch of samples from a \\(\\text{N}(0,1)\\) distribution, compute their confidence intervals, and plot them set.seed(432432432) # How many samples to generate B &lt;- 50 # Sample size of each n &lt;- 20 # Confidence level conf &lt;- .9 # Critical value- the book just gives this as 1.729 # This actually took me a minute to figure out... so make sure # you get what I&#39;m doing here: critval &lt;- qt(1 - (1-conf)/2,df = n-1) # Perform the simulation confints &lt;- 1:B %&gt;% # Generate the samples map(~rnorm(n,0,1)) %&gt;% # Compute the confidence intervals map(~c(&quot;mean&quot; = mean(.x), &quot;lower&quot; = mean(.x) - critval * sd(.x)/sqrt(n), &quot;upper&quot; = mean(.x) + critval * sd(.x)/sqrt(n)) ) %&gt;% # Put them in a dataframe reduce(bind_rows) %&gt;% # Add a row index, for purposes of plotting mutate(id = 1:B) # Compute the proportion that don&#39;t contain zero scales::percent(mean(confints$upper &lt; 0 | confints$lower &gt; 0)) ## [1] &quot;12.0%&quot; # Plot them. Run this code layer by layer # to understand what each part does. confints %&gt;% ggplot(aes(x = id)) + theme_classic() + geom_point(aes(y = mean),pch = 21,colour = &quot;black&quot;,fill = &quot;orange&quot;,size = 1) + geom_errorbar(aes(ymin = lower,ymax = upper),size = .1) + geom_hline(yintercept = 0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) + scale_y_continuous(breaks = seq(-1,1,by=.2)) + coord_flip() + theme(axis.title.y = element_blank(),axis.text.y = element_blank(),axis.ticks.y = element_blank()) + labs(y = &quot;&quot;) Exercise: re-run this experiment several times with different random seeds. What kind of empirical coverage probabilities—the proportion of intervals that don’t contain zero—do you get? What about if you raise the sample size to n = 100? What about if you raise the number of simulations to B = 1000? 9.1.2 Gross calorific value measurements for Osterfeld 262DE27 The Osterfield data is made available with the book. Its file name is misspelled, so be careful: head data/MIPSdata/grosscalOsterfeld.txt wc -l data/MIPSdata/grosscalOsterfeld.txt ## 23.870 ## 23.730 ## 23.712 ## 23.760 ## 23.640 ## 23.850 ## 23.840 ## 23.860 ## 23.940 ## 23.830 ## 23 data/MIPSdata/grosscalOsterfeld.txt Read it in. I’m leaving this as an exericse (note: not because I’m lazy, I still had to write the code. It’s for your “learning” or whatever). You should get the following: glimpse(osterfield) ## Observations: 23 ## Variables: 1 ## $ calorific_value &lt;dbl&gt; 23.870, 23.730, 23.712, 23.760, 23.640, 23.850, … Recreate the confidence interval in the book: # Compute the sample mean and size xbar &lt;- mean(osterfield$calorific_value) n &lt;- nrow(osterfield) # The population standard deviation, and the critical value/confidence level # are given as: sigma &lt;- .1 conf &lt;- .95 # Make sure to UNDERSTAND this calculation: critval &lt;- qnorm(1 - (1-conf)/2) # 1.96 # Compute the interval c( &quot;lower&quot; = xbar - critval * sigma/sqrt(n), &quot;upper&quot; = xbar + critval * sigma/sqrt(n) ) ## lower upper ## 23.74691 23.82865 9.1.3 Gross calorific value measurements for Daw Mill 258GB41 As an exercise, now recreate the confidence interval in the book for the Daw Mill sample. Read the data in from file grosscalDawMill.txt, call it dawmill. You can compute the sample standard deviation and appropriate critical value as follows: s &lt;- sd(dawmill$calorific_value) critval &lt;- qt(1 - (1-conf)/2,df = nrow(dawmill) - 1) You should get: ## lower upper ## 30.95422 31.06896 9.1.4 Bootstrap Confidence Intervals First, let’s simulate a dataset to illustrate this idea and so we can compare the bootstrap and analytical answers. set.seed(43547803) B &lt;- 2000 n &lt;- 5000 # Simulate one dataset ds &lt;- rnorm(n,0,1) # Values conf &lt;- .95 critval &lt;- qnorm(1 - (1 - conf)/2) # Now resample from it and calculate studentized statistics resampledstats &lt;- 1:B %&gt;% map(~sample(ds,n,replace = TRUE)) %&gt;% map(~c(mean(.x) - mean(ds))/(sd(.x)/sqrt(n))) %&gt;% reduce(c) # The confidence limits are obtained from the sample quantiles: conflim &lt;- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2)) # Here&#39;s a plot that illustrates what these look like: tibble(x = resampledstats) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),colour = &quot;black&quot;,fill = &quot;lightgrey&quot;,bins = 100) + geom_vline(xintercept = conflim[1],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + geom_vline(xintercept = conflim[2],colour = &quot;orange&quot;,linetype = &quot;dotdash&quot;) + stat_function(fun = dnorm,args = list(mean = mean(ds),sd = sd(ds)),colour = &quot;blue&quot;) + labs(title = &quot;Resampled student statistics and empirical confidence limits&quot;, subtitle = &quot;A normal distribution (blue curve) fits well&quot;, x = &quot;&quot;,y = &quot;&quot;) I deliberately chose a large sample size and number of bootstrap samples to make the results look good. I encourage you to change these numbers to try and break this simulation. The bootstrap-resampled confidence limits are close to the truth: conflim ## 2.5% 97.5% ## -1.999947 2.005969 qnorm(c((1-conf)/2,1 - (1-conf)/2)) ## [1] -1.959964 1.959964 Let’s apply this to the software data. Oddly, I get different values for the mean, standard deviation, and sample size than the book reports. If you figure out why, I will give you a \\(\\$10\\) Tim card. The differences aren’t meaningful enough to affect the presentation of these ideas. # Read it in: software &lt;- readr::read_csv( file = &quot;data/MIPSdata/software.txt&quot;, col_names = &quot;time&quot;, col_types = &quot;n&quot; ) B &lt;- 1000 # Same as book n &lt;- nrow(software) mn &lt;- mean(software$time) ss &lt;- sd(software$time) conf &lt;- .9 set.seed(821940379) resampledstats &lt;- 1:B %&gt;% map(~sample(software$time,n,replace = TRUE)) %&gt;% map(~c(mean(.x) - mn)/(sd(.x)/sqrt(n))) %&gt;% reduce(c) # The confidence limits are obtained from the sample quantiles: conflim &lt;- quantile(resampledstats,probs = c((1-conf)/2,1 - (1 - conf)/2)) # The confidence interval: c( &quot;lower&quot; = mn + conflim[1] * ss/sqrt(n), &quot;upper&quot; = mn + conflim[2] * ss/sqrt(n) ) ## lower.5% upper.95% ## 474.9540 781.4853 Exercise: compute a \\(90\\%\\) confidence interval for the mean for the software data assuming the data is normally distributed. This does NOT mean that you should use a normal distribution for calculating the critical values– if you don’t understand why, go back and read the “Variance Unknown” section on page 348. I got the following: critval &lt;- qt(1 - (1-conf)/2,df = n -1) c( &quot;lower&quot; = mn - critval * ss/sqrt(n), &quot;lower&quot; = mn + critval * ss/sqrt(n) ) ## lower lower ## 502.0045 796.2749 Does this lead to different conclusions in practice than the bootstrap interval? "]
]
