[
["index.html", "STA238: Probability, Statistics, and Data Analysis Chapter 1 Introduction", " STA238: Probability, Statistics, and Data Analysis Alison Gibbs and Alex Stringer 2019-10-23 Chapter 1 Introduction Data only becomes information after it is analyzed. Analyzing data is extraordinarily difficult, but is the only way to learn meaningfully about the world. In this book, you will learn about analyzing data. You will learn how to make principled, measured statements about the data on hand (descriptive statistics); how to use the data on hand to make statements about the underlying world, and how to quantify the uncertainty in your statements (inferential statistics), and how to use the data on hand to make judgements, with uncertainty quantification, about what data you will see next (predective statistics). "],
["section-supplement-to-chapters-15-and-16.html", "Chapter 2 Supplement to Chapters 15 and 16 2.1 Old Faithful 2.2 Drilling 2.3 Exercises 2.4 Extended example: smoking and age and mortality 2.5 Case study: rental housing in Toronto", " Chapter 2 Supplement to Chapters 15 and 16 This chapter implements much of the analysis shown in chapters 15 and 16 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. 2.1 Old Faithful The Old Faithful data is stored in the oldfaithful.txt file in the book data folder. To read it in to R, first you need to look at its contents. 2.1.1 Read in the data You can either open it in a text editor (since it’s small), or print out the first few lines on the command line. If you plan on ever doing any meaningful work with data, the latter is really your only choice, so let’s do that. If you have a Mac or Linux-based system, open a terminal. If you have a Windows based system, get a Mac or Linux. Just kidding (not really though). Download Git Bash: https://git-scm.com/download/win and open a new terminal window. To look at a text file on the command line, use the head command. You have to tell it where to look on your computer. You can navigate to the folder where the data is stored and just type the name of the file, or you can type a whole filepath, or a relative filepath. Right now, I am in the working directory for this book, and relative to this location, the data is stored in data/MIPSdata. So I type: head data/MIPSdata/oldfaithful.txt ## 216 ## 108 ## 200 ## 137 ## 272 ## 173 ## 282 ## 216 ## 117 ## 261 A column of numbers is printed. This indicates that the text file oldfaithdful.txt contains a single column of numbers, and no header row. A header row is what it’s called when the first row of the dataset contains the names of the columns. To read such a dataset into R, we’ll use the read_delim function in the readr package. The readr package is automatically loaded when we load the tidyverse package, so we’ll just do that: library(tidyverse) oldfaithful &lt;- readr::read_csv( file = &quot;data/MIPSdata/oldfaithful.txt&quot;, # Tell it where the file is col_names = &quot;time&quot;, # Tell it that the first row is NOT column names, and at the same time, tell it what name you want for the column. col_types = &quot;n&quot; # Tell it that there is one column, and it is &quot;numeric&quot; (n) ) # Check what was read in using the dplyr::glimpse() function dplyr::glimpse(oldfaithful) ## Observations: 272 ## Variables: 1 ## $ time &lt;dbl&gt; 216, 108, 200, 137, 272, 173, 282, 216, 117, 261, 110, 235,… By glimpseing the data, we see that the format matches what we saw in the raw file, and we are given the number of rows too. Check that the number of rows matches what was in the raw file by printing out the number of rows on the command line using the wc -l command (“wc” = “word count” and “-l” means count “lines”): wc -l data/MIPSdata/oldfaithful.txt ## 271 data/MIPSdata/oldfaithful.txt What happened, why don’t they match? They do. The wc -l command actually counts the number of “newline” characters in the file. It is customary to end a data file with a newline character. This file doesn’t, though. How do I know? Type the following: tail data/MIPSdata/oldfaithful.txt ## 111 ## 255 ## 119 ## 135 ## 285 ## 247 ## 129 ## 265 ## 109 ## 268 This prints out the last few lines of the file. In this book, these are printed as normal. But if you do it on the command line, you’ll see that the command prompt gets printed on the same line as the final number, 268. This indicates that the file does not end with a newline character, and hence the total number of newlines in the file is 271, corresponding to 272 actual lines of data. Remark: I would wager that you are currently surprised that this was so difficult. Even reading in the simplest possible dataset created subtle challenges that needed to be addressed. There is probabably a reason this stuff isn’t taught as heavily in stats courses as it ought to be; it’s difficult and tedious and not as fun as math. But to do meaningful work using data, you have to get good at this stuff. So it is worth struggling through. 2.1.2 Graphical Summaries Now that the data has been read in and checked, we can realized the fruits of our labour! Chapter 15 analyzes these data using a tabular display, histogram, kernel density estimate, and empirical CDF. Here is how to do these things in R. # Tabular display print(oldfaithful$time) ## [1] 216 108 200 137 272 173 282 216 117 261 110 235 252 105 282 130 105 ## [18] 288 96 255 108 105 207 184 272 216 118 245 231 266 258 268 202 242 ## [35] 230 121 112 290 110 287 261 113 274 105 272 199 230 126 278 120 288 ## [52] 283 110 290 104 293 223 100 274 259 134 270 105 288 109 264 250 282 ## [69] 124 282 242 118 270 240 119 304 121 274 233 216 248 260 246 158 244 ## [86] 296 237 271 130 240 132 260 112 289 110 258 280 225 112 294 149 262 ## [103] 126 270 243 112 282 107 291 221 284 138 294 265 102 278 139 276 109 ## [120] 265 157 244 255 118 276 226 115 270 136 279 112 250 168 260 110 263 ## [137] 113 296 122 224 254 134 272 289 260 119 278 121 306 108 302 240 144 ## [154] 276 214 240 270 245 108 238 132 249 120 230 210 275 142 300 116 277 ## [171] 115 125 275 200 250 260 270 145 240 250 113 275 255 226 122 266 245 ## [188] 110 265 131 288 110 288 246 238 254 210 262 135 280 126 261 248 112 ## [205] 276 107 262 231 116 270 143 282 112 230 205 254 144 288 120 249 112 ## [222] 256 105 269 240 247 245 256 235 273 245 145 251 133 267 113 111 257 ## [239] 237 140 249 141 296 174 275 230 125 262 128 261 132 267 214 270 249 ## [256] 229 235 267 120 257 286 272 111 255 119 135 285 247 129 265 109 268 # Ugly! You can use the View() function to open the data in a spreadsheet # Not run: # View(oldfaithful) # Histogram using base R hist(oldfaithful$time) # Base R graphics are outdated and won&#39;t get you a job/will make your papers # look too classical. Use ggplot2, which is loaded automatically with the tidyverse: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Try different numbers of bins and you might see different patterns. Do you think this is a # good or bad thing, or both? # Their &quot;optimal&quot; bin width: s &lt;- sd(oldfaithful$time) n &lt;- nrow(oldfaithful) b &lt;- (24*sqrt(pi))^(1/3) * s * (n^(-1/3)) b ## [1] 36.89694 oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = round(b),colour = &quot;black&quot;,fill = &quot;blue&quot;,alpha = .3) + labs(title = &quot;Eruption times for Old Faithful geyser&quot;, subtitle = stringr::str_c(&quot;&#39;Optimal&#39; bin width of b = &quot;,round(b)), x = &quot;Eruption time&quot;, y = &quot;Density&quot;) To get kernel density estimates, there are a couple different ways. The density function in R does the math for you, using a Gaussian kernel (the functions \\(K_{i}\\) are taken to be Gaussian density functions with mean and standard deviation determined by the data). You can plot the output of density using base R or ggplot. You can also use the ggplot geom_density function to do something similar automatically. # Kernel density estimation in R dens &lt;- density(oldfaithful$time) dens ## ## Call: ## density.default(x = oldfaithful$time) ## ## Data: oldfaithful$time (272 obs.); Bandwidth &#39;bw&#39; = 20.09 ## ## x y ## Min. : 35.74 Min. :3.771e-06 ## 1st Qu.:118.37 1st Qu.:8.571e-04 ## Median :201.00 Median :2.412e-03 ## Mean :201.00 Mean :3.022e-03 ## 3rd Qu.:283.63 3rd Qu.:5.143e-03 ## Max. :366.26 Max. :8.070e-03 plot(dens) # Okay... ggplot? tibble(x = dens$x,y = dens$y) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Manually-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # Can also do automatically: oldfaithful %&gt;% ggplot(aes(x = time)) + theme_classic() + geom_density() + labs(title = &quot;Kernel density estimate, Old Faithful data&quot;, subtitle = &quot;Automatically-calculated values&quot;, x = &quot;Eruption time&quot;, y = &quot;Density&quot;) # The reason to manually calculate the values is because you have more control. # I don&#39;t know what&#39;s happening at the endpoints there, and it&#39;s too much work # to go and figure out how to make ggplot not do that. # When you calculate the plotting values yourself and then put them into ggplot, # you have total control! For the empirical distribution function, we use the ecdf function in R. You would think this function should behave in a similar manner to the density function, but it doesn’t. It returns a function which computes the ecdf. It still has a plot method, but to use it with ggplot we have to use stat_function: faithful_ecdf &lt;- ecdf(oldfaithful$time) plot(faithful_ecdf) # ggplot tibble(x = c(100,300)) %&gt;% # Tell ggplot we want to plot the ecdf from 100 to 300 ggplot(aes(x = x)) + theme_classic() + stat_function(fun = faithful_ecdf) + labs(title = &quot;Empirical CDF for Old Faithful Eruption Times&quot;, x = &quot;Eruption Time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) Discussion point: the CDF is the integrated pdf: \\[ F(x) = \\int_{-\\infty}^{x}f(s)ds \\] So why don’t we integrate the kernel density estimate to get the empirical CDF? One of the great benefits of taking a computation-forward approach to statistical inference is that we can “shoot first and ask questions later”- just try it, and then (maybe) use math to explain the results. Here is the world’s most naive numerical integration-based estimate of a CDF: tibble( x = dens$x[-1], y = cumsum(dens$y[-1]) * diff(dens$x) # Quick and dirty numerical integration. Can you put something better? ) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_line() + labs(title = &quot;Numerical integration-based empirical CDF for Old Faithful data&quot;, x = &quot;Eruption time&quot;, y = &quot;Empirical probability that an eruption time is less than x&quot;) What do you think? Is this better, worse, or just different than the ECDF? Chapter 16 discusses boxplots. To get a boxplot, again, you can use base R or ggplot. We have: # Base R boxplot(oldfaithful$time) # ggplot oldfaithful %&gt;% ggplot(aes(y = time)) + theme_classic() + geom_boxplot(width = .1) + labs(title = &quot;Boxplot of eruption times, Old Faithful data&quot;, y = &quot;Eruption time&quot;) + # Have to play around with the x axis to get it to look nice *shrug* coord_cartesian(xlim = c(-.2,.2)) + theme(axis.text.x = element_blank()) 2.1.3 Numerical Summaries (Ch 16) The numerical summaries in chapter 16 include the sample mean and median, sample standard deviation and mean absolute deviation, and quantiles. # Mean mean(oldfaithful$time) ## [1] 209.2684 # Median median(oldfaithful$time) ## [1] 240 # Standard deviation sd(oldfaithful$time) ## [1] 68.48329 # Quantiles: tell it which ones you want. I want 0, 25, 50, 75, 100 quantile(oldfaithful$time,probs = c(0,.25,.50,.75,1.00)) ## 0% 25% 50% 75% 100% ## 96.00 129.75 240.00 267.25 306.00 # The zeroth and hundredth quantiles are the sample minimum and maximum: min(oldfaithful$time) ## [1] 96 max(oldfaithful$time) ## [1] 306 # Actually, you can get all this with the summary() function: summary(oldfaithful$time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 96.0 129.8 240.0 209.3 267.2 306.0 # Mean absolute deviation # I don&#39;t know an R function for this off the top of my head (maybe you can find one?) # So let&#39;s calculate it manually. # Actually, let&#39;s calculate them ALL manually! # Mean: sum(oldfaithful$time) / length(oldfaithful$time) ## [1] 209.2684 # Median: it&#39;s the 50th percentile quantile(oldfaithful$time,probs = .5) ## 50% ## 240 # Can get it manually too: sort(oldfaithful$time)[length(oldfaithful$time)/2] ## [1] 240 # Better to use the quantile() function though. # Standard deviation. Need to save the mean to a variable first: mn &lt;- mean(oldfaithful$time) sqrt( sum( (oldfaithful$time - mn)^2 ) / ( length(oldfaithful$time) - 1 ) ) ## [1] 68.48329 # MAD. Similar to sd: md &lt;- median(oldfaithful$time) median( abs(oldfaithful$time - md) ) ## [1] 38.5 # IQR: quantile(oldfaithful$time,probs = .75) - quantile(oldfaithful$time,probs = .25) ## 75% ## 137.5 # Note that there are various ways to correct for the fact that not all quantiles # are exact (you may not have a datapoint which has EXACTLY 25% of the data below it, # like if the sample size isn&#39;t divisible by 4). R probably uses a different method # than the book, so the results here are slightly different. 2.2 Drilling 2.2.1 Read in data Read in the drilling.txt file: head data/MIPSdata/drilling.txt ## 5 640.67 830 ## 10 674.67 800 ## 15 708 711.33 ## 20 735.67 867.67 ## 25 754.33 940.67 ## 30 723.33 941.33 ## 35 664.33 924.33 ## 40 727.67 873 ## 45 658.67 874.67 ## 50 658 843.33 # Use the read_tsv (not csv), because this file is &quot;tab-delimited&quot;; the spaces # between columns contain tab characters. drilling &lt;- readr::read_tsv( file = &quot;data/MIPSdata/drilling.txt&quot;, col_names = c(&quot;depth&quot;,&quot;dry&quot;,&quot;wet&quot;), col_types = &quot;nnn&quot; ) glimpse(drilling) ## Observations: 80 ## Variables: 3 ## $ depth &lt;dbl&gt; 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75,… ## $ dry &lt;dbl&gt; 640.67, 674.67, 708.00, 735.67, 754.33, 723.33, 664.33, 72… ## $ wet &lt;dbl&gt; 830.00, 800.00, 711.33, 867.67, 940.67, 941.33, 924.33, 87… 2.2.2 Graphical summaries To make a scatterplot, you can again use base R or ggplot. We want separate plots for dry and wet holes. You can do this by plotting these data separately, or you can re-format the data and have ggplot do it automatically: # Base R par(mfrow = c(1,2)) # Plots on a 1 x 2 grid plot(dry~depth,data = drilling) plot(wet~depth,data = drilling) # ggplot # Two separate plots: dryplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = dry)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Dry Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) wetplt &lt;- drilling %&gt;% ggplot(aes(x = depth,y = wet)) + theme_classic() + geom_point(pch = 21) + # pch=21 is the magic command to give you hollow points labs(title = &quot;Wet Holes&quot;, x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) cowplot::plot_grid(dryplt,wetplt,nrow = 1) # There is a lot of repeated code here. For a better way to make these two # plots, first create a base plot object and then reuse it: drillingplt &lt;- drilling %&gt;% ggplot(aes(x = depth)) + theme_classic() + labs(x = &quot;Depth&quot;, y = &quot;Mean drill time&quot;) + scale_y_continuous(breaks = seq(500,1300,by = 200)) + coord_cartesian(ylim = c(500,1300)) dryplt2 &lt;- drillingplt + labs(title = &quot;Dry holes&quot;) + geom_point(aes(y = dry),pch = 21) wetplt2 &lt;- drillingplt + labs(title = &quot;Wet holes&quot;) + geom_point(aes(y = wet),pch = 21) cowplot::plot_grid(dryplt2,wetplt2,nrow = 1) # Much better # Another option is to reformat the data and create the plot # with a single command. To do this we stack the dry and wet measurements # on top of each other, and create a new variable which indicates whether the # measurements are dry or wet. This is called putting the data into &quot;long&quot; format. # ggplot then knows how to &quot;facet&quot; the plots according to this new variable. # Check it out: drilling_long &lt;- drilling %&gt;% tidyr::gather(type,time,dry:wet) %&gt;% dplyr::mutate(type = case_when( # Rename the type values for plotting type == &quot;dry&quot; ~ &quot;Dry Holes&quot;, type == &quot;wet&quot; ~ &quot;Wet Holes&quot; )) dplyr::glimpse(drilling_long) ## Observations: 160 ## Variables: 3 ## $ depth &lt;dbl&gt; 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75,… ## $ type &lt;chr&gt; &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry Holes&quot;, &quot;Dry H… ## $ time &lt;dbl&gt; 640.67, 674.67, 708.00, 735.67, 754.33, 723.33, 664.33, 72… drilling_long %&gt;% ggplot(aes(x = depth,y = time)) + theme_classic() + facet_wrap(~type) + geom_point() + labs(title = &quot;Mean drilling times&quot;, x = &quot;Depth&quot;, y = &quot;Mean drilling time&quot;) # Even though there is more overhead with this method, I recommend it because # it scales to more variables. If you wanted to make 20 plots, you&#39;d have to # have 20 plots in the previous method but here, the code is actually identical. To make boxplots, transform the data in the same way as for the side-by-side scatterplots, and give to ggplot: drilling_long %&gt;% ggplot(aes(x = type,y = time)) + theme_classic() + geom_boxplot() + labs(title = &quot;Boxplots for wet and dry drilling times, Drilling data&quot;, x = &quot;Hole Type&quot;, y = &quot;Mean drilling time&quot;) 2.2.3 Numerical summaries We can compute numerical summaries of drilling times in the same way as for the eruption times from the Old Faithful data. However, the drilling data are naturally grouped, so we should compute our summaries by group— i.e. by hole type, wet or dry. To do this requires a bit more machinery; we will operate on a dataframe and use formal grouping operations. This isn’t that much harder, but it’s really powerful. Check it out: drilling_long %&gt;% group_by(type) %&gt;% # Everything that happens now happens separately for Wet Holes and Dry Holes summarize( mean = mean(time), sd = sd(time), min = min(time), median = median(time), quant_25 = quantile(time,probs = .25), quant_75 = quantile(time,probs = .75), max = max(time) ) ## # A tibble: 2 x 8 ## type mean sd min median quant_25 quant_75 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Dry Holes 806. 154. 584 758. 688. 912. 1238 ## 2 Wet Holes 944. 124. 697. 918. 851. 1030. 1238. Grouped operations are fundamental to modern data analysis. 2.3 Exercises The Janka Hardness data is in the file jankahardness.txt. Reproduce the analysis in the book yourself. 2.4 Extended example: smoking and age and mortality You now have some tools. How do you put them to use in practice? Analyzing data “in the wild” involves a lot of decision making, and this can impact the conclusions you make. Consider a famous dataset containing information on smoking and mortality. The data is available in the R package faraway. We may load the package and data and retrieve information on it as follows: # install.packages(&quot;faraway&quot;) # Run this to install the faraway package, which has useful datasets library(faraway) # Attach the faraway package data(&quot;femsmoke&quot;) # Load the &quot;femsmoke&quot; data # ?femsmoke # Run this to open the help page for the dataset. We see from the help page, and associated reference to the paper in the American Statistician, that the data comes from asking women in Whickham, England, whether they smoke or not, and then following up in 20 years to see if they died. In describing the context surrouning the data—a quite natural step to take—we have achieved our first example of a descriptive analysis. We didn’t even use and numbers or code. Just describing what the data is counts. And it’s important, because as we learned in the introduction, information can’t exist without context. To go further in our descriptive analysis, we need some more quantitative pieces of descriptive information. What might we want to know about our data? Some ideas: How many observations are there in the data, and what does an observation represent in the context of how the data was collected? How many variables are present in the data, and what does each variable represent in the context of how the data was collected? How might we summarize each variable? We might compute a mean and a five-number summary for “continuous” variables, and a table of counts for “categorical” variables (more on this later…). Let’s see how we can obtain these descriptive measures in R: # install.packages(&quot;tidyverse&quot;) # install.packages(&quot;SMPracticals&quot;) # For datasets library(tidyverse) # We use a LOT of functions from this package. # Get the number of observations (rows), variables, and an idea # of what the data looks like: glimpse(femsmoke) ## Observations: 28 ## Variables: 4 ## $ y &lt;dbl&gt; 2, 1, 3, 5, 14, 7, 27, 12, 51, 40, 29, 101, 13, 64, 53, 6… ## $ smoker &lt;fct&gt; yes, no, yes, no, yes, no, yes, no, yes, no, yes, no, yes… ## $ dead &lt;fct&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, ye… ## $ age &lt;fct&gt; 18-24, 18-24, 25-34, 25-34, 35-44, 35-44, 45-54, 45-54, 5… # One observation represents a count of people in each category. # How many people? femsmoke %&gt;% summarize(num_people = sum(y)) # The summarize() function lets you compute summaries of variables in your dataframe ## num_people ## 1 1314 # How many smokers? femsmoke %&gt;% filter(smoker == &quot;yes&quot;) %&gt;% # filter() lets you choose which rows to keep summarize(num_smokers = sum(y)) ## num_smokers ## 1 582 # How many non-smokers? femsmoke %&gt;% filter(smoker == &quot;no&quot;) %&gt;% summarize(num_non_smokers = sum(y)) ## num_non_smokers ## 1 732 # We can get both those numbers at the same time: femsmoke %&gt;% group_by(smoker) %&gt;% # group_by() makes summarize() compute summaries within levels of a variable summarize(num_people = sum(y)) ## # A tibble: 2 x 2 ## smoker num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 582 ## 2 no 732 There are lots of other descriptive statistics you could calculate. Summary: descriptive analysis involves communicating properties of a dataset, and the context behind the dataset. 2.4.1 Exercises How many non-smoking 18-24 year olds are there in the femsmoke data? Answer using filter(). How many smokers died? Answer using filter(). How many 45-55 year olds did not die? Compute the following table using group_by() and summarize(): ## # A tibble: 7 x 2 ## age num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 18-24 117 ## 2 25-34 281 ## 3 35-44 230 ## 4 45-54 208 ## 5 55-64 236 ## 6 65-74 165 ## 7 75+ 77 Descriptive analyses are an important task any time you are working with data. However, they don’t accomplish anything new; they just, well, describe what you have. An exploratory analysis involves looking for patterns or structure in the available data. We can use what we find to make decisions or come to conclusions. One thing we can choose to explore about these data is whether we observe any apparent association between smoking and mortality. To investigate any such associations, we can look at the observed mortality rates for smokers and non-smokers. This is exploratory—not descriptive—because we are going beyond simply saying what the data looks like. We are looking for a pattern, namely, “do smokers die more or less frequently than non-smokers?” Exploratory analyses involve decisions made by us, the investigator. They are naturally subjective. Different decisions can lead to uncovering different patterns. For example: # Compute the mortality rate for smokers and non-smokers. # To do this, create a dataframe containing the numbers of smokers # and non-smokers smoker_numbers &lt;- femsmoke %&gt;% # The %&gt;% operator lets you form sequences of operations group_by(smoker) %&gt;% # group_by() makes all the following operations happen within groups summarize(num_people = sum(y)) # Count the number of people who are smokers and not smokers smoker_numbers ## # A tibble: 2 x 2 ## smoker num_people ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 582 ## 2 no 732 # Now, compute the number of people who died out of the smokers and non-smokers # This looks the same as above, except we now filter() only the people who died. smoker_numbers_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% # Retains rows where dead == &quot;yes&quot; only group_by(smoker) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_dead ## # A tibble: 2 x 2 ## smoker num_dead ## &lt;fct&gt; &lt;dbl&gt; ## 1 yes 139 ## 2 no 230 # Now, we join these two tables together and compute the mortality rates by group. smoker_numbers %&gt;% inner_join(smoker_numbers_dead,by = &quot;smoker&quot;) %&gt;% # Joins rows with the same value of &quot;smoker&quot; mutate(mort_rate = num_dead/num_people) # mutate() creates a new variable, which can be a function of the other variables in the dataframe. ## # A tibble: 2 x 4 ## smoker num_people num_dead mort_rate ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yes 582 139 0.239 ## 2 no 732 230 0.314 See anything interesting? What went wrong? Why are we observing that smokers have a lower mortality rate than non-smokers? This contradicts the context surrounding this analysis, which in this case is the large body of formal and anecdotal evidence suggesting that smoking is harmful to health. Did we make a mistake? One thing we definitely did was ignore some present information. Specifically, we also know how old the women were. How can we include this information in our exploratory analysis? We can compute mortality rates by age: smoker_numbers_age &lt;- femsmoke %&gt;% group_by(smoker,age) %&gt;% # Now we&#39;re grouping by smoker AND age. The rest of the code remains unchanged. summarize(num_people = sum(y)) smoker_numbers_age_dead &lt;- femsmoke %&gt;% filter(dead == &quot;yes&quot;) %&gt;% group_by(smoker,age) %&gt;% summarize(num_dead = sum(y)) smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) ## # A tibble: 14 x 5 ## # Groups: smoker [2] ## smoker age num_people num_dead mort_rate ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yes 18-24 55 2 0.0364 ## 2 yes 25-34 124 3 0.0242 ## 3 yes 35-44 109 14 0.128 ## 4 yes 45-54 130 27 0.208 ## 5 yes 55-64 115 51 0.443 ## 6 yes 65-74 36 29 0.806 ## 7 yes 75+ 13 13 1 ## 8 no 18-24 62 1 0.0161 ## 9 no 25-34 157 5 0.0318 ## 10 no 35-44 121 7 0.0579 ## 11 no 45-54 78 12 0.154 ## 12 no 55-64 121 40 0.331 ## 13 no 65-74 129 101 0.783 ## 14 no 75+ 64 64 1 Older people are more likely to die within the 20 year followup period. However, examining the raw counts of people in each group, we also see that in these data, older people are less likely to smoke than younger people. So in these data, less smokers died, because less smokers were old, and more old people died. But was our first analysis wrong? No. Our first analysis was fine: we computed the mortality rate in each group. The problem was in the reporting, or the way we told the story. We didn’t provide enough information when we said “the mortality rate for smokers was lower than for non-smokers”. We should have mentioned that this is averaging over all age groups. Even when we include age in the analysis, we ought to mention the fact that there are a whole lot of variables we could have measured but didn’t, and we are implicitly averaging over these too. Before moving on, get some practice doing exploratory analysis with the following exercises: 2.4.2 Exercises What is the relative risk of mortality—the ratio of the mortality rates—for smoking 18-24 year olds vs non-smoking 18-24 year olds? Compute the answer manually by reading the numbers off the above table. Then compute it using R by doing the following: Create two datasets using filter(): one containing smokers and one containing non-smokers. filter() out only the 18-24 year olds. This gives you two datasets each with only one row. For example, smokers &lt;- femsmoke %&gt;% filter(smoker == &quot;yes&quot;,age = &quot;18-24&quot;). inner_join() the two datasets together, using age as the by variable: smokers %&gt;% inner_join(???,by = &quot;age&quot;) Advanced: modify the above steps to create the following table of relative mortality rates. You should start from a cleaned up version of the mortality rate by age table: rates_by_age &lt;- smoker_numbers_age %&gt;% inner_join(smoker_numbers_age_dead,by = c(&quot;smoker&quot;,&quot;age&quot;)) %&gt;% mutate(mort_rate = num_dead/num_people) %&gt;% ungroup() # The data was previously grouped, we don&#39;t want this anymore Use dplyr::select() to remove and rename columns, see ?dplyr::select. ## # A tibble: 7 x 4 ## age smoker_mort_rate nonsmoker_mort_rate relative_risk ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18-24 0.0364 0.0161 2.25 ## 2 25-34 0.0242 0.0318 0.760 ## 3 35-44 0.128 0.0579 2.22 ## 4 45-54 0.208 0.154 1.35 ## 5 55-64 0.443 0.331 1.34 ## 6 65-74 0.806 0.783 1.03 ## 7 75+ 1 1 1 2.5 Case study: rental housing in Toronto The RentSafeTO: Apartment Building Standards program is designed to help renters in the city of Toronto make informed choices about where to live, and to enforce a minimum standard of quality upon rental units within the city. With rents skyrocketing and home ownership not a reasonable option for most, having an informed view of the rental market is imperative for Toronto residents. It also helps keep leaders accountable, specifically if we focus on social and community housing buildings. Comprehensive and fairly clean data from the program, along with specific information, is available at https://open.toronto.ca/dataset/apartment-building-evaluation/. Data for the following were downloaded on 2019/09/16. To start your analysis, go now and download the data and open it in a spreadsheet and have a look. Familiarize yourselves with the variable descriptions and how the data were collected; the documentation. This somewhat tedious task is a first step of any data analysis, in academia, industry, government, or wherever. 2.5.1 Load the data The data are stored in a .csv file, which stands for “comma-separated-values”. Storing data in a text file with a separator, usually a comma, is very common. These are referred to as “flat files” in an industrial context, to distinguish them from data stored in databases. We may read the data into R using the read_csv function in the readr package. The readr package is part of the tidyverse package that we used before, so if you installed that package, you have it loaded. # https://open.toronto.ca/dataset/apartment-building-evaluation/ # install.packages(&quot;readr&quot;) # Read the data in. This means call the readr::read_csv() function, point it # to where you saved the data on your computer, and then save the result to a # variable. I am naming this variable &#39;apartmentdata&#39;. # Type ?readr::read_csv if you want to read about this function. apartmentdata &lt;- readr::read_csv( file = &quot;data/apartment-data/toronto-apartment-building-evaluations.csv&quot; ) ## Parsed with column specification: ## cols( ## .default = col_double(), ## EVALUATION_COMPLETED_ON = col_character(), ## PROPERTY_TYPE = col_character(), ## RESULTS_OF_SCORE = col_character(), ## SITE_ADDRESS = col_character(), ## WARD = col_character() ## ) ## See spec(...) for full column specifications. The message displayed is telling you that readr::read_csv() guessed at what kind of data were in each column, i.e. numbers, letters, dates, etc. You should make sure, as I have while writing, that these are what you expect. You can get a concise view of this dataset using the glimpse function in the dplyr package, which is automatically loaded when you load the tidyverse: glimpse(apartmentdata) ## Observations: 3,446 ## Variables: 32 ## $ `_id` &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1… ## $ BALCONY_GUARDS &lt;dbl&gt; NA, NA, NA, NA, 5, NA, 5, 3, 4, 4, 3… ## $ CONFIRMED_STOREYS &lt;dbl&gt; 28, 4, 3, 3, 29, 3, 7, 18, 17, 32, 4… ## $ CONFIRMED_UNITS &lt;dbl&gt; 457, 15, 26, 10, 272, 12, 95, 287, 3… ## $ ELEVATORS &lt;dbl&gt; 4, NA, NA, NA, 5, NA, 5, 4, 5, 4, NA… ## $ ENTRANCE_DOORS_WINDOWS &lt;dbl&gt; 3, 3, 3, 4, 5, 4, 4, 4, 3, 4, 4, 3, … ## $ ENTRANCE_LOBBY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 4, 4, 4, 4, 4, 4, … ## $ EVALUATION_COMPLETED_ON &lt;chr&gt; &quot;04/03/2019&quot;, &quot;05/24/2018&quot;, &quot;07/11/2… ## $ EXTERIOR_CLADDING &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 5, 4, 4, 3, 3, 4, … ## $ EXTERIOR_GROUNDS &lt;dbl&gt; 3, 4, 3, 3, 5, 4, 5, 4, 3, 4, 3, 4, … ## $ EXTERIOR_WALKWAYS &lt;dbl&gt; 3, 5, 4, 4, 5, 4, 5, 4, 3, 4, 4, 3, … ## $ GARBAGE_BIN_STORAGE_AREA &lt;dbl&gt; 3, 4, 3, 3, 4, 3, 3, 3, 4, 4, 4, 4, … ## $ GARBAGE_CHUTE_ROOMS &lt;dbl&gt; 3, NA, NA, NA, 5, NA, 5, 4, 3, 4, 5,… ## $ GRAFFITI &lt;dbl&gt; 5, 5, 5, 5, 5, 4, 5, 4, 3, 4, 5, 5, … ## $ INTERIOR_LIGHTING_LEVELS &lt;dbl&gt; 3, 4, 4, 4, 5, 4, 4, 3, 3, 4, 3, 4, … ## $ INTERIOR_WALL_CEILING_FLOOR &lt;dbl&gt; 4, 3, 4, 4, 5, 4, 4, 3, 4, 4, 4, 3, … ## $ INTERNAL_GUARDS_HANDRAILS &lt;dbl&gt; 3, 4, 3, 4, 5, 4, 5, 4, 4, 4, 5, 4, … ## $ NO_OF_AREAS_EVALUATED &lt;dbl&gt; 18, 14, 14, 13, 19, 16, 17, 18, 19, … ## $ OTHER_FACILITIES &lt;dbl&gt; 4, NA, NA, NA, 5, NA, NA, NA, NA, NA… ## $ PARKING_AREA &lt;dbl&gt; 2, NA, NA, NA, 4, 3, 5, 2, 4, 4, 2, … ## $ PROPERTY_TYPE &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SO… ## $ RESULTS_OF_SCORE &lt;chr&gt; &quot;Evaluation needs to be conducted in… ## $ RSN &lt;dbl&gt; 4365723, 4364249, 4408585, 4288126, … ## $ SCORE &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, … ## $ SECURITY &lt;dbl&gt; 4, 3, 3, 4, 5, 4, 5, 3, 4, 4, 4, 4, … ## $ SITE_ADDRESS &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL D… ## $ STAIRWELLS &lt;dbl&gt; 4, 4, 3, 4, 5, 4, 5, 4, 4, 4, 3, 4, … ## $ STORAGE_AREAS_LOCKERS &lt;dbl&gt; NA, NA, NA, NA, NA, 4, NA, NA, 3, 4,… ## $ WARD &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, … ## $ WATER_PEN_EXT_BLDG_ELEMENTS &lt;dbl&gt; 4, 4, 4, 4, 5, 4, 5, 4, 5, 3, 3, 4, … ## $ YEAR_BUILT &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, … ## $ YEAR_REGISTERED &lt;dbl&gt; 2018, 2018, 2018, 2017, 2018, 2017, … That’s bigger than the smoking data! 3,446 rental apartment buildings, each with 32 factors measured. The buliding’s address and Ward number are in there, which are helpful for characterizing neighbourhoods. 2.5.2 Analysis I: what does the data look like? As a first step, we want to get an idea of what our data “looks like”. This typically means picking some interesting variables and summarizing their distributions somehow. Which variables to pick will depend on the context. Often it will be clear which variables are important, and sometimes not. Because you read the documentation and familiarized yourselves with the variables in the dataset, you know that there is a variable called SCORE which sums up the individual category scores for each building. In the context of determining building quality, this seems like an important variable to look at. We’ll summarize the distribution of SCORE using a five-number summary and mean, and a histogram with a kernel density estimate. First, prepare the data for analysis: # First, select only the columns you want # This isn&#39;t strictly necessary but trust me, it makes # debugging WAY easier. # I&#39;m also renaming the columns so the dataframe looks prettier. # Again, trust me. This stuff matters. apartmentclean &lt;- apartmentdata %&gt;% filter(!is.na(SCORE)) %&gt;% # Remove apartments with missing scores dplyr::select(ward = WARD, score = SCORE, property_type = PROPERTY_TYPE, year_built = YEAR_BUILT, address = SITE_ADDRESS ) glimpse(apartmentclean) # Much nicer! ## Observations: 3,437 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;0… ## $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57… ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;,… ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 19… ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP… To compute the five-number summary (plus mean), use the summary() function in R. I also want to know the standard deviation of SCORE: summary(apartmentclean$score) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 37.00 68.00 72.00 72.28 77.00 99.00 sd(apartmentclean$score,na.rm = TRUE) ## [1] 7.117172 The worst building in the city has a total score of 37, and the best gets 99. The median score—half the buildings in the city have a lower score, and half a higher score than this—is 72, and this roughly equals the mean of 72.28. 25% of buildings score higher than 77, and 25% score lower than 68. So most buildings seem to fall within less than one standard deviation of the mean, which indicates that these data are fairly concentrated about their mean. To provide some context, go look up your own building (if you live in a rental building) or that of a friend in the data. Where does your building fall in terms of quality within Toronto? So far we have used tabular displays to summarize our data, for both the smoking and the apartment data. We also learned about graphical displays. Let’s see a histogram of the scores, with a kernel density estimate: We can make a histogram in R as follows: # The ggplot2 package is loaded as part of the tidyverse score_histogram &lt;- apartmentclean %&gt;% ggplot(aes(x = score)) + # Tell ggplot to use score on the x axis theme_classic() + # Make the plot pretty geom_histogram( # Makes a histogram aes(y = ..density..), bins = 20, colour = &quot;black&quot;, fill = &quot;lightgrey&quot; ) + geom_density() + labs(title = &quot;Distribution of RentSafeTO Apartment Building Standards score&quot;, x = &quot;Score&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(30,100,by = 5)) score_histogram It appears that most buildings are in the 65 to 85 range. I actually just moved from a building that has a 66 to a building that has an 86. The difference is substantial! 2.5.3 Analysis II: Do different wards have different quality housing? A Ward is an administrative district within the city that has a single city counsellor. If I’m thinking about moving to, or within, Toronto, I want to know: Do different wards have different quality housing?. In order to address this question we need to decide on the following: Variable of interest. How do we quantify our research question? We need to pick a measure of quality. Picking different measures can lead to different conclusions. Filters. Do we look at all apartment buildings? Should we look only at those built after, or before, a certain date? Only those that meet a certain minimum, or maximum, standard of quality according to our definition? Are there any other kinds of decisions we might have to consider? Methods. What kind of statistical tools should we use to address our research question? We need to pick descriptive statistics to report, and decide whether we want to include other auxillary variables in the analysis. Conclusions. How do we report our results? Tables, charts, maps? Should we include subjective, editorial commentary, or let the data speak for themselves? This is already overwhelming! Let’s make an attempt at it. I propose: Our variable of interest should be SCORE, which you know (because you read the documentation…) is the “overall score of the buliding”. Higher is better. The actual formula is included in the documentation of the data. We will filter the data to only include buildings where PROPERTY_TYPE == 'PRIVATE', which will restrict our analysis to not include social housing. The quality of social housing is an important social justice issue (that you will investigate in the exercises) but it’s somewhat separate (?) from the question of where to look for rental housing. Our methods will include looking at a table of average scores for each ward. We will also look at whether older or newer buildings receive better scores. We will summarize our conclusions through a subjective assessment of the above table of average scores. With these decisions made, we may proceed with our analysis using the tidyverse as follows: # Apply filter(s). apartmentfiltered &lt;- apartmentclean %&gt;% filter(property_type == &quot;PRIVATE&quot;) # When filtering, always compare the filtered and unfiltered data to ensure # the result is as expected: glimpse(apartmentclean) ## Observations: 3,437 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;04&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;0… ## $ score &lt;dbl&gt; 71, 77, 71, 78, 98, 76, 93, 72, 74, 78, 73, 76, 57… ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;SOCIAL HOUSING&quot;,… ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 1920, 2017, 1967, 2015, 1970, 19… ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP… glimpse(apartmentfiltered) ## Observations: 2,873 ## Variables: 5 ## $ ward &lt;chr&gt; &quot;04&quot;, &quot;19&quot;, &quot;11&quot;, &quot;07&quot;, &quot;03&quot;, &quot;17&quot;, &quot;17&quot;, &quot;08&quot;, &quot;1… ## $ score &lt;dbl&gt; 71, 77, 71, 98, 76, 93, 72, 74, 78, 73, 76, 57, 70… ## $ property_type &lt;chr&gt; &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVATE&quot;, &quot;PRIVA… ## $ year_built &lt;dbl&gt; 1976, 1953, 1948, 2017, 1967, 2015, 1970, 1976, 19… ## $ address &lt;chr&gt; &quot;2350 DUNDAS ST W&quot;, &quot;9 STAG HILL DR&quot;, &quot;130 MACP… nrow(apartmentclean) - nrow(apartmentfiltered) # Dropped 567 rows. ## [1] 564 # Now create the table of averages: apartmentfiltered %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score)) ## # A tibble: 26 x 2 ## ward avg_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 01 71.5 ## 2 02 73.0 ## 3 03 70.5 ## 4 04 68.2 ## 5 05 71.7 ## 6 06 72.1 ## 7 07 69.8 ## 8 08 73.5 ## 9 09 67.5 ## 10 10 72.2 ## # … with 16 more rows Bah! What happened? Why are there these NA values? NA is the value R uses to mean “missing”. We have to hope that whether a rental apartment building’s score is missing is not related to what that score is, that is, we hope apartments with higher or lower scores aren’t missing more often. We will ignore missingness for now. To do this, use the na.rm = TRUE option in mean: apartmentsummary &lt;- apartmentfiltered %&gt;% group_by(ward) %&gt;% summarize(avg_score = mean(score,na.rm = TRUE)) apartmentsummary ## # A tibble: 26 x 2 ## ward avg_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 01 71.5 ## 2 02 73.0 ## 3 03 70.5 ## 4 04 68.2 ## 5 05 71.7 ## 6 06 72.1 ## 7 07 69.8 ## 8 08 73.5 ## 9 09 67.5 ## 10 10 72.2 ## # … with 16 more rows This isn’t a super friendly way of comparing these 26 numbers. I’d rather use a graphical display, like the boxplots we learned about in chapters 15 and 16: apartmentfiltered %&gt;% ggplot(aes(x = ward,y = score)) + theme_classic() + geom_boxplot() + labs(title = &quot;ABS score shows moderate variability across wards in Toronto&quot;, x = &quot;Ward&quot;, y = &quot;ABS Score&quot;) It looks like some wards are better than others. Or are they? Can we make any definitive conclusions based on this? 2.5.4 Analysis III: trends in quality over time Let’s go further and analyze some other interesting aspects of these data. I’m interested in knowing: Are newer buildings higher quality? We have the score and the year_built, and we’d like to investigate whether newer buildings (higher year_built) have higher scores. We have another decision to make. We could consider year_built to be a categorical variable, and make a bar chart. Or, we could consider it to be a continuous variable. Because values of year_built are inherently comparable, and because our research question involves making such comparisons, we will consider year_built to be a continuous variable. One type of plot used to compare continuous variables is a scatterplot. A scatterplot has continuous variables on the x- and y-axes, and draws a point (or bubble) at each place in the two-dimensional plane where a datapoint occurs. We can make this kind of plot in ggplot2 as well. This time, we use the raw (well, cleaned and filtered) data: apartmentfiltered %&gt;% filter(year_built &gt; 1900) %&gt;% ggplot(aes(x = year_built,y = score)) + theme_classic() + geom_point(pch = 21,colour = &quot;black&quot;,fill = &quot;grey&quot;) + # pch=21 makes the bubbles hollow, looks nice scale_x_continuous(breaks = seq(1900,2020,by=10)) + # Set the x-axis range labs(title = &quot;Less rental buildings are being built recently, but they are of higher quality&quot;, x = &quot;Year Built&quot;, y = &quot;ABS Score&quot;) Very interesting. You can clearly see the baby boom of the 1950’s to 1970’s, followed by a massive slowdown in construction during the economic slump in the 1980’s, and a complete stop when rent control was introduced in 1991 (remember, these are rental buildings only). Then, we see a new wave of rental building construction, and the new buildings seem to be of higher quality. What are the highest and lowest quality rental buildings in Toronto? # Get the 10 highest scoring buildings apartmentfiltered %&gt;% arrange(desc(score)) %&gt;% # Sort the data, descending, by score slice(1:10) # Take the first ten- i.e. the top ten ## # A tibble: 10 x 5 ## ward score property_type year_built address ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 YY 99 PRIVATE 2018 561 SHERBOURNE ST ## 2 17 99 PRIVATE 2017 123 PARKWAY FOREST DR ## 3 16 99 PRIVATE 1963 70 PARKWOODS VILLAGE DR ## 4 07 98 PRIVATE 2017 2 VENA WAY ## 5 17 97 PRIVATE 1968 24 FOREST MANOR RD ## 6 12 96 PRIVATE 1960 42 GLEN ELM AVE ## 7 13 95 PRIVATE 2017 252 VICTORIA ST ## 8 02 95 PRIVATE 1969 500 SCARLETT RD ## 9 16 95 PRIVATE 1962 67 PARKWOODS VILLAGE DR ## 10 07 95 PRIVATE 2016 6 VENA WAY Wow. I know where I want to live. 2.5.5 Summary We have seen how even something simple like trying to figure out whether different areas of the city have different quality housing can require a lot of decision making. And these decisions require expertise. By taking a principled approach to learning data analysis, you are empowering yourself to live a life that is better informed. But notice that we didn’t really answer any questions in this chapter. We saw some rough patterns, but were they real? If we made different decisions, or if we sampled different data, would we have seen different patterns? In order to understand what the problem is and how to approach it, we need to take a more detailed look at the concept of error. 2.5.6 Exercises Take each of the analyses we have performed on the Toronto rental data and say whether you think it’s descriptive, exploratory, or prescriptive, or a mix, and say why. What is that “YY” ward that shows up in the dot plot? Investigate this unusual observation. Read the documentation online and choose three variables that you find the most interesting. Reproduce the analyses I, II and III using your variables. Is there more or less variability across wards than with score? What is the ward with the highest average score? In what ward is/are the building(s) with the highest score(s)? Is this the same ward, or not? Would you expect the ward with the highest average to also have the highest-scoring buildings? Repeat this question with the lowest scoring buildings instead of the highest. If you live in a rental apartment, find it in these data. If not, find a friend’s place. How does your building compare to other buildings in your ward? Does it score higher or lower? The filter() function is your friend here, or you can use apartmentfiltered %&gt;% arrange(SITE_ADDRESS) %&gt;% print(n = Inf) and then find yours in the list manually. Combine the analyses of sections 2.3.2 and 2.3.3. with that of 2.3.4. Specifically, make a table and a boxplot of the average score by year. This means replace ward by year_built in the analysis of sections 2.3.2. and 2.3.3. Do your conclusions change when comparing with 2.3.4? Why or why not? Would you expect this to always be the case? Advanced: analyze the quality of social housing in Toronto. Perform a similar analysis to what we performed here for PROPERTY_TYPE == 'PRIVATE', but instead for PROPERTY_TYPE %in% c('SOCIAL HOUSING','TCHC') (equivalent to PROPERTY_TYPE != 'PRIVATE'). Does the quality of social housing in Toronto vary greatly across different wards? Is it improving or degrading over time? Do you think we have enough information here to definitively answer these questions? "],
["section-supplement-to-chapters-13-and-14.html", "Chapter 3 Supplement to Chapters 13 and 14 3.1 Law of Large Numbers (Chapter 13) 3.2 Central Limit Theorem (Chapter 14)", " Chapter 3 Supplement to Chapters 13 and 14 This chapter implements much of the analysis shown in chapters 13 and 14 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. 3.1 Law of Large Numbers (Chapter 13) Load the packages we need: library(tidyverse) To simulate from a \\(\\text{Gamma}(\\alpha,\\beta)\\) distribution in , use the rgamma function: # Simulate from a gamma # Type ?rgamma to get information on the parametrization # There are three named parameters: shape, scale, and rate. Rate = 1/scale. # The distribution has mean shape * scale, or shape / rate. # The parametrization in the book is the &quot;rate&quot; parametrization. # Always read the docs to understand what the parameters are. # # Simulate with shape = 2 and rate = 1, so mean = 2 and variance = ? (exercise) rgamma(1,shape = 2,rate = 1) ## [1] 0.3130447 # The first &quot;1&quot; in the call gives the number of values to simulate: rgamma(10,shape = 2,rate = 1) ## [1] 4.3955044 2.1181364 1.1471783 1.2415639 1.8213429 1.3588531 2.9468860 ## [8] 1.1871804 0.9114808 1.3696986 Plot the density of the gamma sample mean for various \\(n\\), recreating the left side of Figure 13.1: # Define a function to compute the density # Fix the scale and shape arguments at defaults of what&#39;s used in the book # You can play around with these. gamma_samplemean_density &lt;- function(x,n,shape = 2,rate = 1) { # x: point to evaluate the density at # n: sample size # Just use the dgamma function dgamma(x,shape = n * shape,rate = n*rate) } # Plot it for various n # Define a function to make the plot plot_for_n &lt;- function(n) { # Create a function with the n argument fixed densfun &lt;- purrr::partial(gamma_samplemean_density,n = n) # Plot using ggplot tibble(x = c(0,4)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = densfun) + coord_cartesian(xlim = c(0,4),ylim = c(0,1.5)) + labs(title = stringr::str_c(&quot;n = &quot;,n)) } # Create the plots purrr::map(c(1,2,4,9,16,400),plot_for_n) %&gt;% cowplot::plot_grid(plotlist = .,nrow = 3) Simulate the running average of experiments of size \\(n\\) from a \\(\\text{Gamma}(2,1)\\) for \\(n = 1,\\ldots,500\\) and plot them, recreating Figure 13.2: set.seed(54768798) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 500 alpha &lt;- 2 beta &lt;- 1 gamma_experiment &lt;- rgamma(n = n,shape = alpha,rate = beta) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(gamma_experiment) / 1:length(gamma_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = alpha / beta,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) What happens when you increase the number? Try it for \\(n = 1,000, n = 10,000\\), and so on. Now try this for the Cauchy distribution, the left panel of Figure 13.3: set.seed(4235) # So I can reproduce these results # Simulate one experiment of size 500 n &lt;- 500 mu &lt;- 2 sigma &lt;- 1 cauchy_experiment &lt;- rcauchy(n = n,location = mu,scale = sigma) # Compute the running average- a vector where the nth component is the average # of the first n terms in gamma_experiment runningaverage &lt;- cumsum(cauchy_experiment) / 1:length(cauchy_experiment) # Plot, remembering that the true mean is 2 / 1 = 2 tibble(x = 1:length(runningaverage), y = runningaverage) %&gt;% ggplot(aes(x = x,y = y)) + theme_classic() + geom_point(pch = &quot;.&quot;) + geom_hline(yintercept = alpha / beta,colour = &quot;red&quot;,size = .5,linetype = &quot;dotted&quot;) Yikes! The Cauchy distribution is the worst. Exercise: repeat this for the Pareto distribution, recreating the right panel of Figure 13.3. You can simulate from the Pareto distribution using the rpareto function in the actuar package. Type install.packages(&quot;actuar&quot;) and then actuar::rpareto. Type ?actuar::rpareto to get help on using this function. Figuring out how to use the function is part of the exercise. 3.1.1 Extended example: the probability of heads As an extended example, consider trying to figure out what the probability of heads is for a fair coin, just based on flipping the coin a bunch of times. We can use the material of Section 13.4 to address this challenge. Let \\(X\\) be a random variable which takes values \\(0\\) and \\(1\\) if the coin comes up tails or heads on any given flip. Let \\(C = \\left\\{ 1\\right\\}\\), so the specific event that we are interested in is whether the coin comes up heads on any given flip. \\(p = \\text{P}(X\\in C)\\) is hence the probability of heads. To estimate \\(p\\) using the LLN, we will take a coin which has probability \\(p\\) of coming up heads and flip it a bunch of times and calculate the proportion of flips that come up heads. # Function to flip the coin n times, and return a sequence of 1 if heads and 0 if tails # for each flip. # Use the rbinom function to simulate from a bernoulli/binomial distribution. flip_the_coin &lt;- function(n,p) { # n: number of times to flip the coin. # p: probability of heads # Returns a vector of length n containing the results of each flip. rbinom(n,1,p) } # Function to flip the coin n times and compute the # sample proportion of heads sample_proportion_of_heads &lt;- function(n,p) { # Returns a number representing the sample proportion of heads # in n flips mean(flip_the_coin(n,p)) } # Try it out: sample_proportion_of_heads(10,.5) ## [1] 0.4 Exercise: create a plot of the running average of sample proportions of heads, similar to the above plots for the Gamma and Cauchy (Figure 13.3). How many times do you think you need to flip the coin before the result is an accurate estimate? Does this change for different values of \\(p\\)? 3.2 Central Limit Theorem (Chapter 14) Let’s investigate the scaling power on \\(n\\) when standardizing averages. The book shows in Figure 14.1 that multiplying \\((\\bar{X} - \\mu)\\) by \\(n^{1/4}\\) isn’t enough to stabilize the variance in the distribution, \\(n^{1}\\) is too much, and \\(n^{1/2}\\) is just right. Exercise: derive the probability density of \\(Y = n^{p}(\\bar{X} - \\mu)\\) when \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\text{Gamma}(2,1)\\). Hint: look at the code below. How do I compute this? What formula am I using? We can recreate Figure 14.1 as follows: scalingdensity &lt;- function(y,n,p) { dgamma( x = y*n^(-p) + 2, shape = 2 * n, rate = n ) * n^(-p) } plotscalingdensity &lt;- function(n,p) { dens &lt;- purrr::partial(scalingdensity,n = n,p = p) tibble(x = c(-3,3)) %&gt;% ggplot(aes(x = x)) + theme_classic() + stat_function(fun = dens) + coord_cartesian(xlim = c(-3,3),ylim = c(0,.4)) + labs(title = stringr::str_c(&quot;n = &quot;,n,&quot;; scaling = &quot;,p)) + theme(text = element_text(size = 8)) } # Plot them all! This code is somewhat advanced; you should run it # line by line and figure out what each step does. It&#39;s a pretty concise # way of doing a lot of operations. This type of &quot;list comprehension&quot; is fundamental # to learning to program with data. pltlist &lt;- expand.grid(n = c(1,2,4,16,100), p = c(1/4,1/2,1)) %&gt;% dplyr::arrange(n,p) %&gt;% as.list() %&gt;% purrr::transpose() %&gt;% purrr::map(~plotscalingdensity(n = .x[[&quot;n&quot;]],p = .x[[&quot;p&quot;]])) cowplot::plot_grid(plotlist = pltlist, nrow = 5,ncol = 3) You can play around with different scaling values and sample sizes. Figure 14.2 is like the centre column of Figure 14.1 with a normal density curve overlayed. Exercise: recreate the left column of Figure 14.2 by doing the following: Replace expand.grid(n = c(1,2,4,16,100),p = c(1/4,1/2,1)) by expand.grid(n = c(1,2,4,16,100),p = c(1/2)) in the above code that generates the plots (and set the values of nrow and ncol appropriately as well). Add a normal density line. You have to modify the plotscalingdensity. Add a layer as follows: stat_function(fun = dnorm,linetype = &quot;dotted&quot;). We can compute probabilities involving the standard normal distribution function in R using the pnorm function. Let’s compute the approximate probability described in a couple of the textbook’s examples, and compare it to simulated and true values. # &quot;Did we have bad luck?&quot;. # The actual probability is (why?): n &lt;- 500 1 - pgamma(2.06,shape = 2*n,rate = 1*n) ## [1] 0.1710881 # The CLT probability is: 1 - pnorm(.95) ## [1] 0.1710561 # Why are we doing 1 - pnorm()? pnorm() gives P(X &lt; x) for X ~ N(0,1) # To get P(X &gt; x), you can do pnorm(.95,lower.tail = FALSE) ## [1] 0.1710561 # but it&#39;s easier to just do P(X &gt; x) = 1 - P(X &lt; x) # We can also simulate this probability by generating a bunch of gamma random # samples and seeing how often their averages are &gt; 2.06: N &lt;- 10000 exceeded &lt;- numeric(N) for (i in 1:N) { samp &lt;- rgamma(n,shape = 2,rate = 1) mn &lt;- mean(samp) exceeded[i] &lt;- as.numeric(mn &gt; 2.06) } mean(exceeded) ## [1] 0.1714 # Pretty good. Can you experiment with the number of simulations? How # many experiments do you need to simulate in order to get an accurate value? 3.2.1 Extended example: the probability of heads In our coin example from the LLN section, we investigated how many flips were needed to get an average number of heads that was close (in probability) to the true probability of heads. Using the CLT, we can get a probabilistic quantification of the error rate– how far away from the truth the proportion of heads is likely to be. Similar to the LLN experiment, we’re goin to flip the coin a bunch of times and calculate the sample proportion of heads; then we’re going to do that a bunch of times and plot a histogram of the sample proportions. The CLT tells us that as long as each experiment has enough flips, the resulting probability density of the sample proportion of heads should be approximately Normal. N &lt;- 1000 # Number of experiments to do n &lt;- 100 # Number of times to flip the coin in each experiment p &lt;- .5 # True probability of heads experiments &lt;- numeric(N) for (i in 1:N) { experiments[i] &lt;- sample_proportion_of_heads(n,p) } # Plot them tibble(x = experiments) %&gt;% ggplot(aes(x = x)) + theme_classic() + geom_histogram(aes(y = ..density..),bins=30,colour=&quot;black&quot;,fill=&quot;orange&quot;,alpha = .5) + stat_function(fun = dnorm,args = list(mean = p,sd = sqrt(p*(1-p)/n)),colour = &quot;purple&quot;) + labs(title = &quot;Empirical distribution of sample proportions of heads&quot;, subtitle = stringr::str_c(&quot;# of flips: &quot;,n,&quot;, true probability of heads: &quot;,p), x = &quot;Proportion of heads&quot;, y = &quot;Empirical Density&quot;) + scale_x_continuous(breaks = seq(0,1,by=.1)) Exercises: 1. Recreate the above plot with \\(n = 10, 50, 100, 1000\\) and \\(p = .4, .2, .8, .01, .99\\). What do you see? Is the accuracy of the normal approximation to this distribution affected by \\(n\\) or \\(p\\)? 1. What are the mean and variance of the distribution of sample proportions of heads? (Hint: what are the mean and variance of a \\(\\text{Binom}(n,p)\\) random variable?) 1. Recreate the above plot, but scale the sample proportion of heads appropriately such that it has mean \\(0\\) and variance \\(1\\). 1. Recreate the “Normal Approximation of the Binomial Distribution” calculations using The CLT The exact values, using the rbinom function (look up the help file) A simulation "],
["section-supplement-to-chapters-17-and-19.html", "Chapter 4 Supplement to Chapters 17 and 19 4.1 Statistical models (Chapter 17) 4.2 Unbiased Estimators (Chapter 19)", " Chapter 4 Supplement to Chapters 17 and 19 This chapter implements much of the analysis shown in chapters 17 and 19 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. 4.1 Statistical models (Chapter 17) Most of the material from chapter 17 is a review of that from chapters 15 and 16, but in an important new context. You should be doing all exercises from chapter 17 using R, using the tools you have learned from chapters 15 and 16. Here we will focus on the linear regression model. This chapter doesn’t cover how these models are estimated, so here we will show how to use R to plot regression lines. We’ll use ggplot2 to recreate Figure 17.8, and show how to add “smooth” non-linear regression lines to plots as well. 4.1.1 Janka Hardness data The Janka Hardness dataset was already discussed in Chapter 15/16. Since you did the exercises from those chapters prescribed in this supplementary book, you have already read these data into R: head data/MIPSdata/jankahardness.txt ## 24.7 484 ## 24.8 427 ## 27.3 413 ## 28.4 517 ## 28.4 549 ## 29 648 ## 30.3 587 ## 32.7 704 ## 35.6 979 ## 38.5 914 By printing it out on the command line, you can tell that the file is tab-delimited. Use readr::read_delim() to read it in: library(tidyverse) janka &lt;- readr::read_delim( file = &quot;data/MIPSdata/jankahardness.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;density&quot;,&quot;hardness&quot;), col_types = &quot;nn&quot; ) glimpse(janka) ## Observations: 36 ## Variables: 2 ## $ density &lt;dbl&gt; 24.7, 24.8, 27.3, 28.4, 28.4, 29.0, 30.3, 32.7, 35.6, 3… ## $ hardness &lt;dbl&gt; 484, 427, 413, 517, 549, 648, 587, 704, 979, 914, 1070,… Create a scatterplot with ggplot2: jankascatter &lt;- janka %&gt;% ggplot(aes(x = density,y = hardness)) + theme_classic() + geom_point() + scale_x_continuous(breaks = seq(20,80,by=10)) + scale_y_continuous(breaks = seq(0,3500,by=500)) + coord_cartesian(xlim = c(20,80),ylim = c(0,3500)) + labs(x = &quot;Wood density&quot;, y = &quot;Hardness&quot;) jankascatter To add a line to a plot, use geom_abline(). We can use this to recreate the regression line from Figure 17.8: jankascatter + geom_abline(slope = 57.51,intercept = -1160.5) But how did the book calculate these values? We’ll answer this question in a later chapter. But for now, it would still be nice to get the computer to compute these values for us rather than typing them in manually. We can do this using the geom_smooth() function in ggplot2: jankascatter + geom_smooth(method = &quot;lm&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) The “lm” stands for “linear model” and the “se” stands for “standard error”; leaving this at its default of “TRUE” would add error bars to the line, which we’ll learn about later (give it a try though). We can also add a non-linear curve to the plot using this technique. Most of the approaches to doing non-linear regression involve breaking the data up into small chunks based on the x-axis values, and then doing linear regression in each chunk and joining the resulting lines. The “loess” non-linear regression line is an example of this approach. It roughly stands for “local regression and smoothing splines”. We can add this using ggplot2 as well: jankascatter + geom_smooth(method = &quot;loess&quot;,se = FALSE,colour = &quot;black&quot;,size = .5) See how it’s a bit more wiggly, but still pretty straight? The data really does look like it supports a linear relationship between density and hardness. This is not common in modern practice! 4.1.2 Extended example: TTC ridership revenues Toronto’s population is growing over time. This puts strain on our outdated public transit system. But it should also lead to increased revenues. According to (https://globalnews.ca/news/1670796/how-does-the-ttcs-funding-compare-to-other-transit-agencies/)[a news article from a few years back], the TTC is the least-subsidized major transit agency in North America, which means that its operating budget is the most dependent on fare revenue out of any in all of the US and Canada. Tracking how ridership revenues are changing over time is very important. The city does do this. Go to (https://www.toronto.ca/city-government/data-research-maps/toronto-progress-portal/)[ the City of Toronto Progress Portal] and type “TTC” and click on the box that says “TTC Ridership Revenues” to see a report. You can download the data from here, but since it’s a bit tricky to describe exactly how, I have posted the file ttc-ridership-revenues.csv on Quercus. We are going to read these data into R and analyze the relationship between year and revenue. If you’re thinking “that sounds really easy, we just did that!”… just keep reading. First, print the data out and count the number of rows on the command line: head data/ttc-ridership-revenues.csv wc -l data/ttc-ridership-revenues.csv ## Year,Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec ## 2007 YTD Actual,$70600000,$131200000,$204600000,$264900000,$322000000,$395100000,$452100000,$507500000,$585600000,$646900000,$712500000,$774700000 ## 2008 YTD Actual,$72700000,$137600000,$217500000,$278200000,$340600000,$419600000,$482400000,$544100000,$629000000,$696400000,$766600000,$837000000 ## 2009 YTD Actual,$69300000,$135400000,$216600000,$280500000,$344000000,$422100000,$483400000,$543500000,$627200000,$693900000,$762400000,$834900000 ## 2010 YTD Actual,$72200000,$143400000,$230700000,$302400000,$372500000,$459100000,$528800000,$595700000,$689100000,$764500000,$842000000,$929300000 ## 2011 YTD Actual,$75300000,$150800000,$244400000,$318300000,$392400000,$484800000,$557300000,$625500000,$722000000,$799500000,$879100000,$969900000 ## 2012 YTD Actual,$75500000,$154800000,$253900000,$331600000,$408300000,$507100000,$581800000,$654300000,$755800000,$835900000,$919100000,$1017600000 ## 2013 YTD Actual,$93200000,$176600000,$278200000,$360300000,$439700000,$539700000,$617400000,$693500000,$799600000,$882500000,$968900000,$1052100000 ## 2014 YTD Actual,$92200000,$178900000,$284400000,$367200000,$449700000,$552300000,$633200000,$712200000,$822000000,$907900000,$998200000,$1086500000 ## 2015 YTD Actual,$90600000,$178200000,$284100000,$370500000,$455300000,$559600000,$641700000,$721400000,$833200000,$920800000,$1011600000,$1107300000 ## 14 data/ttc-ridership-revenues.csv Yikes! Real data is messy. This data isn’t even that messy and it still seems messy. We see that the file is comma-separated and has a header. The first column is text and the others are… well, they’re supposed to be numeric, but they are stored in the file with dollar signs. WHY! This kind of thing is super annoying and super common. We could remove the dollar signs from the text file directly using sed or a similar UNIX-based tool, but I prefer whenever possible to keep all my analysis on one platform. We’ll read it into R as-is and then parse and change datatypes there: # Read in the data ridership &lt;- readr::read_csv( file = &quot;data/ttc-ridership-revenues.csv&quot;, col_names = TRUE, # Tells readr to read the column names from the first line of the file. col_types = stringr::str_c(rep(&quot;c&quot;,13),collapse = &quot;&quot;) # Read all 13 columns as &quot;c&quot;haracter ) glimpse(ridership) ## Observations: 13 ## Variables: 13 ## $ Year &lt;chr&gt; &quot;2007 YTD Actual&quot;, &quot;2008 YTD Actual&quot;, &quot;2009 YTD Actual&quot;, &quot;2… ## $ Jan &lt;chr&gt; &quot;$70600000&quot;, &quot;$72700000&quot;, &quot;$69300000&quot;, &quot;$72200000&quot;, &quot;$75300… ## $ Feb &lt;chr&gt; &quot;$131200000&quot;, &quot;$137600000&quot;, &quot;$135400000&quot;, &quot;$143400000&quot;, &quot;$1… ## $ Mar &lt;chr&gt; &quot;$204600000&quot;, &quot;$217500000&quot;, &quot;$216600000&quot;, &quot;$230700000&quot;, &quot;$2… ## $ Apr &lt;chr&gt; &quot;$264900000&quot;, &quot;$278200000&quot;, &quot;$280500000&quot;, &quot;$302400000&quot;, &quot;$3… ## $ May &lt;chr&gt; &quot;$322000000&quot;, &quot;$340600000&quot;, &quot;$344000000&quot;, &quot;$372500000&quot;, &quot;$3… ## $ Jun &lt;chr&gt; &quot;$395100000&quot;, &quot;$419600000&quot;, &quot;$422100000&quot;, &quot;$459100000&quot;, &quot;$4… ## $ Jul &lt;chr&gt; &quot;$452100000&quot;, &quot;$482400000&quot;, &quot;$483400000&quot;, &quot;$528800000&quot;, &quot;$5… ## $ Aug &lt;chr&gt; &quot;$507500000&quot;, &quot;$544100000&quot;, &quot;$543500000&quot;, &quot;$595700000&quot;, &quot;$6… ## $ Sep &lt;chr&gt; &quot;$585600000&quot;, &quot;$629000000&quot;, &quot;$627200000&quot;, &quot;$689100000&quot;, &quot;$7… ## $ Oct &lt;chr&gt; &quot;$646900000&quot;, &quot;$696400000&quot;, &quot;$693900000&quot;, &quot;$764500000&quot;, &quot;$7… ## $ Nov &lt;chr&gt; &quot;$712500000&quot;, &quot;$766600000&quot;, &quot;$762400000&quot;, &quot;$842000000&quot;, &quot;$8… ## $ Dec &lt;chr&gt; &quot;$774700000&quot;, &quot;$837000000&quot;, &quot;$834900000&quot;, &quot;$929300000&quot;, &quot;$9… This does not look like it’s in a form ready to analyze. Some problems: The Year has unwanted text in it. We just want the number representing what year it is. The revenue is stored across 12 columns, one for each month. We want the annual revenue for our analysis. The actual numeric revenue is stored as text with a dollar sign. We need to parse out the number part and convert to a numeric datatype before we can analyze it. Problems 1 and 3 require a bit of text parsing; Problem 2 requires converting from “wide” to “long” format. Let’s do it: # PROBLEM 1: Year # To parse out only the number part, use a regular expression. # Our string starts with a four digit number which starts with 20. We want to capture this number # and nothing else. # The ^ means &quot;the start of the string&quot;. # The [20]{2} means &quot;a 0 or a 2, exactly twice&quot; # The [0-9]{2} means &quot;anything from 0 - 9, exactly twice&quot; year_regex &lt;- &quot;^[20]{2}[0-9]{2}&quot; # Use stringr::str_extract to extract a substring matching the regular expression: stringr::str_extract(&quot;2007 YTD Actual&quot;,year_regex) ## [1] &quot;2007&quot; # PROBLEM 2: wide to long # Use the tidyr::gather() function for &quot;gather&quot;ing columns and putting them # into one column: ridership %&gt;% tidyr::gather(month,revenue,Jan:Dec) ## # A tibble: 156 x 3 ## Year month revenue ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2007 YTD Actual Jan $70600000 ## 2 2008 YTD Actual Jan $72700000 ## 3 2009 YTD Actual Jan $69300000 ## 4 2010 YTD Actual Jan $72200000 ## 5 2011 YTD Actual Jan $75300000 ## 6 2012 YTD Actual Jan $75500000 ## 7 2013 YTD Actual Jan $93200000 ## 8 2014 YTD Actual Jan $92200000 ## 9 2015 YTD Actual Jan $90600000 ## 10 2016 YTD Actual Jan $90800000 ## # … with 146 more rows # PROBLEM 3: removing the dollar sign # Again, use text matching. Because $ is itself a special character, # to match it, you have to &quot;escape&quot; it using a backslash dollar_regex &lt;- &quot;\\\\$&quot; # Remove matching strings using stringr::str_remove() stringr::str_remove(&quot;$1234&quot;,dollar_regex) ## [1] &quot;1234&quot; # Now, combine all these into one data cleaning pipeline. # Remember we have monthly revenue, so to get yearly revenue, we sum # over months. ridership_clean &lt;- ridership %&gt;% tidyr::gather(month,revenue,Jan:Dec) %&gt;% # &quot;transmute&quot; is like mutate, but it deletes all original columns transmute(year = stringr::str_extract(Year,year_regex), revenue = stringr::str_remove(revenue,dollar_regex)) %&gt;% mutate_at(c(&quot;year&quot;,&quot;revenue&quot;),as.numeric) %&gt;% # Turn both year and revenue into numeric variables group_by(year) %&gt;% # Sum revenue for each year to get yearly revenue summarize(revenue = sum(revenue)) %&gt;% filter(year &lt; 2019) # 2019 has incomplete data, so remove it glimpse(ridership_clean) ## Observations: 12 ## Variables: 2 ## $ year &lt;dbl&gt; 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 20… ## $ revenue &lt;dbl&gt; 5067700000, 5421700000, 5413200000, 5929700000, 62193000… That looks a lot better! As usual, you should run each line of code one by one to understand what is happening. Because we went to the effort of cleaning the data, we can now plot it easily: ridershipscatter &lt;- ridership_clean %&gt;% ggplot(aes(x = year,y = revenue)) + theme_classic() + geom_point() + labs(title = &quot;Annual ridership revenues for the TTC&quot;, x = &quot;Year&quot;, y = &quot;Revenue&quot;) + scale_y_continuous(labels = scales::dollar_format()) # Make the y-axis pretty ridershipscatter Add a linear and non-linear regression line: leftplot &lt;- ridershipscatter + geom_smooth(method = &quot;lm&quot;,size = .5,se = FALSE,colour = &quot;black&quot;) + labs(subtitle = &quot;Linear regression line&quot;) rightplot &lt;- ridershipscatter + geom_smooth(method = &quot;loess&quot;,size = .5,se = FALSE,colour = &quot;black&quot;) + labs(subtitle = &quot;Non-linear regression line&quot;,y = &quot;&quot;) cowplot::plot_grid(leftplot, rightplot + theme(axis.text.y = element_blank()), # Take away the second plot&#39;s y-axis nrow=1) Exercise: re-do this analysis but don’t sum over month. This will give 12 points per year on the plot. Do one linear regression per month. Recreate the following plot yourself: To do this, you have to create a dataset that looks like this: ## Observations: 144 ## Variables: 3 ## $ month &lt;chr&gt; &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, &quot;Jan&quot;, … ## $ revenue &lt;dbl&gt; 70600000, 72700000, 69300000, 72200000, 75300000, 755000… ## $ year &lt;dbl&gt; 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 20… You should make the following modifications: Replace transmute with mutate so you don’t delete the month column. Replace aes(x = year,y = revenue) with aes(x = year,y = revenue,group = month,colour = month) in the call to ggplot. Don’t sum over months. 4.2 Unbiased Estimators (Chapter 19) Unbiasedness is one property of an estimator that the book claims is attractive. Let’s investigate this by recreating some of their simulations. 4.2.1 Simulated network data Figure 19.1 shows histograms of simulations of samples \\(X_{1},\\ldots,X_{n}\\) of size \\(n=30\\) from a \\(X\\sim\\text{Poisson}(\\log 10)\\) distribution. We want to estimate the parameter \\(p_{0}\\), which is the probability that \\(X = 0\\): \\[\\begin{equation} p_{0} = P(X = 0) = e^{-\\lambda} \\end{equation}\\] where \\(\\lambda = E(X) = \\log 10\\) in this example. The book suggests two estimators, \\(S = \\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0)\\) and \\(T = e^{-\\bar{X}_{n}}\\). \\(S\\) corresponds to calculating the sample proportion of times \\(X_{i}=0\\), and \\(T\\) corresponds to estimating the population mean \\(\\lambda\\) using the sample mean \\(\\bar{X}_{n}\\) and then plugging this in to the actual formula for the value \\(p_{0}\\). These both come from somewhere, and we’ll see this when we talk about Maximum Likelihood. For now, just take them as both being candidates for an estimator of \\(p_{0}\\). Let’s investigate their sampling distributions by recreating the simulation from the book: set.seed(6574564) # Simulate 500 random samples of size 30 from a poisson(log(10)) # You need the purrr package, part of the tidyverse, for the map() function N &lt;- 500 n &lt;- 30 lambda &lt;- log(10) p0 &lt;- exp(-lambda) # True value of p0 # Simulate the samples samplelist &lt;- 1:N %&gt;% map(~rpois(n,lambda)) # Write functions to compute each estimator compute_S &lt;- function(samp) mean(samp == 0) compute_T &lt;- function(samp) exp(-mean(samp)) # Compute them and then store the results in a tibble() estimators &lt;- samplelist %&gt;% map(~c(S = compute_S(.x),T = compute_T(.x))) %&gt;% reduce(bind_rows) glimpse(estimators) ## Observations: 500 ## Variables: 2 ## $ S &lt;dbl&gt; 0.10000000, 0.16666667, 0.16666667, 0.13333333, 0.03333333, 0.… ## $ T &lt;dbl&gt; 0.08774387, 0.12245643, 0.13089846, 0.14956862, 0.08774387, 0.… # Create the plots plt_S &lt;- estimators %&gt;% ggplot(aes(x = S)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) plt_T &lt;- estimators %&gt;% ggplot(aes(x = T)) + theme_classic() + geom_histogram(colour = &quot;black&quot;,fill = &quot;transparent&quot;,bins = 7) + coord_cartesian(ylim = c(0,250)) + geom_vline(xintercept = p0,colour = &quot;red&quot;,linetype = &quot;dotdash&quot;) cowplot::plot_grid(plt_S,plt_T,nrow=1) # Compute the mean of each: estimators %&gt;% summarize_all(mean) ## # A tibble: 1 x 2 ## S T ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.101 0.103 Exercise: \\(S\\) is unbiased and \\(T\\) is biased, as shown in the book. Which estimator would you prefer? Compute a five number summary for \\(S\\) and \\(T\\) from our simulations, recreating the following: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.06667 0.10000 0.10140 0.13333 0.26667 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.03122 0.08209 0.10026 0.10346 0.12246 0.23069 Do you see any meaningful differences? Do the sampling distributions of \\(S\\) and \\(T\\) concentrate around \\(p0\\) in the same way? Now, compute the mode (most frequently-observed value) of \\(S\\) and \\(T\\). You should get the following: mode_S &lt;- estimators %&gt;% group_by(S) %&gt;% summarize(cnt = n()) %&gt;% arrange(desc(cnt)) %&gt;% slice(1) %&gt;% pull(S) mode_T &lt;- estimators %&gt;% group_by(T) %&gt;% summarize(cnt = n()) %&gt;% arrange(desc(cnt)) %&gt;% slice(1) %&gt;% pull(T) cat(&quot;The mode of S is &quot;,mode_S,&quot;\\n&quot;) ## The mode of S is 0.06666667 cat(&quot;The mode of T is &quot;,mode_T,&quot;\\n&quot;) ## The mode of T is 0.09697197 (You’re going to have to figure out how to compute a mode in R. That’s part of the exercise). What do you think about this? Does this contradict \\(S\\) being unbiased and \\(T\\) being biased? Does it change your opinion about which is a better estimator? Finally, compute the square root of the average squared distance of \\(S\\) and \\(T\\) from the true value \\(p_{0}\\). You should get: ## # A tibble: 1 x 2 ## S T ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0550 0.0299 We will see shortly that this quantity, the mean-squared error of an estimator, is much more representative of the quality of an estimator than the bias on its own. Note, of course, that since it depends on the true value, it can’t ever be computed in a real data analysis. It’s a mathematical construct. "],
["section-supplement-to-chapter-20.html", "Chapter 5 Supplement to Chapter 20 5.1 Efficiency and Mean Square Error (Chapter 20)", " Chapter 5 Supplement to Chapter 20 This chapter implements much of the analysis shown in chapter 20 of A Modern Introduction to Probability and Statistics. R code is given for the simple textbook datasets used in the book, and then the concepts are illustrated on real data. All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/. 5.1 Efficiency and Mean Square Error (Chapter 20) This chapter compares estimators using the Mean Squared Error (MSE). The motivating example is estimating the number of German tanks using their observed serial numbers, assuming their serial numbers are assigned uniformly at random. Two estimators are used: one based on the sample mean, and one based on the sample maximum. First, let’s write functions to compute these two estimators, and use simulation to verify that they are unbiased. At this point in the course, you should start feeling comfortable approaching this yourself. I encourage you to try this before looking at my answer as follows: library(tidyverse) # Functions to compute the estimators T1 &lt;- function(x) 2 * mean(x) - 1 T2 &lt;- function(x) ( (length(x) + 1)/length(x) ) * max(x) - 1 # Now, simulate in order to assess their bias. # This goes as follows (try this yourself before looking): # - Choose a true value of N, the parameter to be estimated # - Draw a sample of size n from 1:N without replacement # - Compute T1 and T2 # - Repeat this M times, and compare the average of T1 and T2 to N. N &lt;- 1000 n &lt;- 10 M &lt;- 2000 # Run the simulations. Use the sample.int() function to generate from a DISCRETE # uniform distribution out &lt;- 1:M %&gt;% map(~sample.int(N,n)) %&gt;% map(~c(&quot;T1&quot; = T1(.x),&quot;T2&quot; = T2(.x))) %&gt;% reduce(bind_rows) # What do you expect the mean to be? out %&gt;% summarize_all(mean) ## # A tibble: 1 x 2 ## T1 T2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1004. 998. # Why does this seem to indicate that T1 and T2 have zero bias? # Recreate the plots in Figure 20.1: leftplot &lt;- out %&gt;% ggplot(aes(x = T1)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) rightplot &lt;- out %&gt;% ggplot(aes(x = T2)) + theme_classic() + geom_histogram(aes(y = ..density..),bins = 30,colour = &quot;black&quot;,fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(300,700,1000,1300,1600)) + coord_cartesian(xlim = c(300,1600)) Why does \\(T2\\) seem to have a maximum possible value? Can you compute this mathematically? "],
["section-supplement-to-evans-rosenthal-section-7-1.html", "Chapter 6 Supplement to Evans &amp; Rosenthal Section 7.1 6.1 Example: Bayesian coin flipping", " Chapter 6 Supplement to Evans &amp; Rosenthal Section 7.1 This chapter is a supplement to Chapter 7: Bayesian Inference, section 1: The Prior and Posterior Distributions. This is to support the Bayesian Inference sections of STA238. 6.1 Example: Bayesian coin flipping In this example we will implement an interactive illustration of the relationship between the prior and sample size, and the posterior, for the Beta-Bernoulli example from section 7.1. The purpose is to let you get a feel for how the data updates your beliefs about the parameters in Bayesian inference, and how this relationship depends on how much data you have and how strong your beliefs were to start. Go to the app: https://awstringer1.shinyapps.io/bayesian-tutorial/ The app lets you flip coins and estimate the probability of heads using Frequentist and Bayesian methods. We haven’t covered estimation yet, but we have covered the model for coin flipping in both contexts now, so you should be able to tell what’s happening. Also shown are interval estimates, which measure the strength of the conclusions about \\(p\\) that are made based on the data and model. Narrower interval estimates mean we’re more sure about the value of \\(p\\), after seeing the data. The app lets you change the following: The number of times you flip the coin, The true probability of heads, \\(p\\), Your prior belief about the probability of heads, the “prior mean”, and The strength of your prior beliefs, as measured by the prior standard deviation. Lower standard deviation means you’re more sure about the value of \\(p\\), before seeing any flips. You should answer the following questions: How many flips do you need before the Bayesian and frequentist inferences agree closely? Does this depend on the true value of \\(p\\), your prior belief, and the strength of your prior belief? Intuitively: why are the Bayesian interval estimates narrower than the frequentist ones? Is this always the case? Can you “break” the Bayesian answer by expressing really strong and wrong prior beliefs? Can you “fix” it by flipping the coin more times? "]
]
